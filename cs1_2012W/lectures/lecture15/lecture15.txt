# Lab Assignment 3

[Lab Assignment 3](../../labs/quicksort/lab_quicksort.html) is now
posted, and it's due next Monday, February 20.

# Short Assignment 10

[Short Assignment 10](../../shortassign/file/sa_file.html) is posted
and due on Wednesday.

# Merge sort, continued

Last time, we saw the idea of linear-time merging, but we have not yet
seen the code.  We also briefly saw the code for merge sort, but we'll
take a look at it again.

### Linear-time merging

Recall the idea of linear-time merging: we have two sorted sublists,
and we want to merge them together into a single sorted sublist.  We
repeatedly take the smallest untaken item from each of the two lists
and copy it into the output list.  Eventually, we have taken all the
items from one of the two lists, and at that point, we can just copy
all the remaining items in the other list into the output list.

Look at the `merge` function in [merge_sort.py](merge_sort.py):

~~~{.python}
# merge_sort.py
# Merge sort example for cs1
# Devin Balkcom
# October, 2011
# Modified by THC.

# Take two sorted lists, the_list[p : q+1] and the_list[q+1 : r+1],
# and merge them into the_list[p : r+1].
def merge(the_list, p, q, r):
    # Make a copy of the list items.
    left = the_list[p : q+1]
    right = the_list[q+1 : r+1]
 
    # Until we've gone through one of left and right, move
    # the smallest unmerged item into the next position in
    # the_list[p : r+1].
    
    i = 0       # index into left
    j = 0       # index into right
    k = p       # index into the_list[p : r+1]
    
    while i < len(left) and j < len(right):
        if left[i] < right[j]:
            the_list[k] = left[i]
            i += 1
        else:
            the_list[k] = right[j]
            j += 1
        k += 1
    
    # We've gone through one of left and right entirely.
    # Copy the remainder of the other to the end of the_list[p : r+1].
    
    # If left has remaining items, copy them into the_list, using list slices.
    if i < len(left):
        the_list[k : r+1] = left[i:]
    
    # If right has remaining items, copy them into the_list, using list slices.
    if j < len(right):
        the_list[k : r+1] = right[j:]
        
# Sort the_list[p : r+1], using merge sort.
def merge_sort(the_list, p = None, r = None):
    # If using the default parameters, sort the entire list.
    if p == None and r == None:
        p = 0
        r = len(the_list) - 1
        
    if p < r:   # nothing to do if the sublist has fewer than 2 items
        q = (p + r) / 2                 # midpoint of p and r
        merge_sort(the_list, p, q)      # recursively sort the_list[p : q+1]
        merge_sort(the_list, q+1, r)    # recursively sort the_list[q+1 : r+1]
        merge(the_list, p, q, r)        # and merge them together

l = [19, 18, 24, 72, 16, 49, 13, 12, 1, 66]
merge_sort(l)
print l
~~~

Notice that we can copy part of one list into another using **slice
notation**, that is the colon within the square brackets.  So this
line:

~~~{.python}
the_list[k : r+1] = left[i:]
~~~

copies the items in `left` starting at `i` into `the_list`, starting
at index `k`.

The merge function merges the sorted sublists `the_list[p : q+1]`
(containing the items from index `p` through index `q`&mdash;remember
that with slice notation, the index after the colon is 1 greater than
the index of the last item in the sublist) and and `the_list[q+1 :
r+1]` (containing the items from index `q+1` through index `r`) into
the sorted sublist `the_list[p : r+1]`.  Notice that here the lists
being merged are actually two consecutive sublists of `the_list`, and
that they are being merged into the same locations, indices `p`
through `r`, that they started in.  The `merge` function creates two
new temporary lists, `left` and `right`, copied from the original
list.  It then merges back into the original list.  It repeatedly
chooses the smaller of the unmerged items in `left` and `right` and
copies it back into `the_list`.  The index `i` always tells us the
where to find the next unmerged item in `left`; `j` indexes the next
unmerged item in `right`, and `k` indexes where the next merged item
will go in `the_list`.  Eventually, we exhaust either `left` or
`right`.  At that time, the other list still has unmerged items, and
we can just copy them back into `the_list` directly.

Because the `merge` function spends a constant amount of time to fill
each item in the sublist, it runs in time linear in the size of the
sublist.  That is, if the sublist `the_list[p : r+1]` contains $n$
items, then `merge` runs in time $O(n)$.

### Sorting recursively

Recall that we saw the divide-and-conquer paradigm last time, and we
also saw how merge sort uses it to sort a sublist `the_list[p : r+1]`:

1. **Divide** by finding the index `q` of the midpoint of the sublist
`the_list[p : r+1]`.

2. **Conquer** by recursing on the sublists `the_list[p : q+1]` and
`the_list[q+1 : r+1]`.  This step sorts the sublist `the_list[p :
q+1]`, and it sorts the sublist `the_list[q+1 : r+1]`.

3. **Combine** by merging the sorted sublist `the_list[p : q+1]` and
`the_list[q+1 : r+1]` to produce the sorted sublist `thelist[p :
r+1]`.  We merge by calling the `merge` function.

The recursion "bottoms out" when the sublist to be sorted has fewer
than 2 items, since a sublist with 0 or 1 items is already sorted.

The `merge`_sort function shows how easy divide-and-conquer is for
merge sort.  It almost seems like cheating!  But if you understand
recursion, you'll believe that it works.

If you don't understand how the recursion works&mdash;I mean *really*
understand it&mdash;I recommend that you

* use the debugging tool to step through the code,

* draw the stack frames in the call stack as you step through the code
  by hand,

* draw out the recursion tree for an example list, labeling each node
  by the values of the parameters `p` and `r` to `merge_sort`.

## Sorting a list of objects

The merge sort function we saw is fine for sorting lists of items that
can be easily compared, like strings or ints.  What if we wanted to
sort a list that contained addresses of (references to) objects?  How
does Python know which object is "less than" another?

If you are using the Python `.sort` method, it turns out that you can
write a special method `__lt__` in the class definition.  `__lt__` is
then used to compare the objects when you use the less-than symbol <.

But here, we're not using the Python `.sort` method.  We're writing
our own sorting code.  How can we specify how we want to compare
objects?

Imagine that you have a list of planets.  You might wish to sort by
mass, by distance from the sun, or by mean orbital velocity.  These
quantities might be stored in instance variables of a `Planet` object,
or they might be computable using instance variables of a `Planet`
object.

There is only one place in the merge sort code where we compare items
from the list:

~~~{.python}
        if left[i] < right[l]:
~~~

We need a function to replace the `<`.  Let's say we had two `Planet`
objects, each with a `mass` instance variable.  We could write the
function like this:

~~~{.python}
def lessthan_mass(body1, body2):
    return body1.mass < body2.mass
~~~

I included "mass" in the function name to make it clear that this
"less than" function compares masses.  Somehow I need to specify to
the merge sort code that `merge` should use the `lessthan_mass`
function instead of a simple `<` sign in comparisons.  Fortunately, we
can pass a function as a parameter to another function.  We rewrite
the header for `merge_sort` like this:

~~~{.python}
def merge_sort(the_list, p, r, compare_fn)
~~~

and for `merge`:

~~~{.python}
def merge(the_list, p, q, r, compare_fn):
~~~

When `merge_sort` is called, we just pass in `lessthan_mass` (itself a
function) as the second parameter.  Within `merge_sort`, we change the
call to `merge` to include `compare_fn` as the second actual
parameter.  Finally, in the body of `merge`, we can use `compare_fn`
to compare items rather than using the `<` sign:

~~~{.python}
        if compare_fn(left[i], right[j]):
~~~

## The recursion tree for merge sort

I find drawing the recursion tree to be one of the most helpful ways
to understand or debug a recursive function.  The recursion tree can
be used to easily show:

1.  What functions each function calls, and how the problem is broken
into smaller problems.
2.  The order functions are called in.
3.  The base cases of the recursion, at the leaves.

Here is the recursion tree for merge sort:

![](merge_tree.png)

The calls to `merge` are in red.  The order of the function calls is
shown by small numbers above and to the left of each function call.
We can also see the parameters of the functions.  To save space, I
abbreviated calls to `merge_sort` by `msort` and ommited the name of
the list as a parameter.

You should be able to reconstruct the same tree by just reading and
analyzing the code for merge sort, and you should be able to draw the
recursion tree for any new recursive code you see or write.

## Analyzing runtime for recursive function calls

The recursion tree is also often a very good way to analyze the
running time of a recursive algorithm.  In the following picture, I've
used dashed lines to separate the recursion tree for a sample call to
`merge_sort` into four layers.

![](merge_runtime.png)

The top layer contains the initial call to `merge_sort` and its call
to `merge`.  The second layer contains the two merge sorts of
half-size lists.  The third layer contains the four merge sorts of
quarter-size lists.  The fourth layer contains the eight merge sorts
of eighth-size lists.

Let's analyze the running time of each layer.  A call to `merge_sort`
has two types of costs:

1. The cost associated with all lines of code except function calls to
`merge` or `merge_sort`.  Let's call this a constant, $c$.
2. The cost associated with calls to `merge` and `merge_sort`.

When we account for the runing time, we'll charge the second types of
cost to the functions where the work is actually done.  For example,
when we compute the cost of the top layer, we won't include the costs
incurred by layer 2.

* **Layer 1 (top layer).**  There is one call to `merge_sort`.  The
    charge for that is $c$.  There is one call to `merge`.  If we
    analyze the running time of `merge`, the cost is some constant,
    $k$, times the length of the sublist.  So `merge(0, 7)` costs $8
    k$ time, or $n k$ time.  The overall cost of the layer is $1
    \times (c + nk)$.

* **Layer 2.** There are two calls to `merge_sort`.  The charge for
    that is $2 c$.  There are two calls to `merge`.  The charges are
    each $n/2 \times k$, for a total "merging charge" of $n k$.  The
    overall cost of the layer is $2c + nk$.

* **Layer 3.** Four calls to `merge_sort`: $4 c$.  Four calls to
    `merge`, each of size $n/4$, giving a cost of $n k$.  Total cost:
    $4c + nk$.

* **Layer 4.** Eight calls to `merge_sort`s: $8 c$.  No merges.  Total
    cost: $n c$.

Overall, we see that, not counting the cost of merging, the $2 n - 1$
calls to `merge_sort` cost $(2 n - 1) c$ time.  Now let's look at the
`merge` costs.  Each layer of the tree costs the same for the
`merges`: later layers of the tree had more merges, but the merges
were smaller.  Layers 1, 2, and 3 cost $n k$ each.  How many layers is
the tree?  Since we divide the sublist in half for each new layer,
there are $1 + \log_2 n$ layers in the tree.  The last layer doesn't
have a merge, and so the total merge cost is $n k (\log_2 n)$.

Our total cost for merge sort is therefore $(2 n - 1) c + (\log_2 n) n
k$.  We see that the fastest growing term is $n k\: \log_2 n$;
dropping the constants, we get that the running time of merge sort is
$O(n\: \log_2 n)$.  When the base of the logarithm is a constant, such
as 2, we can drop that, too: $O(n\: \log\: n)$.

# Linked lists

For the next few weeks, we will be looking at how to implement and use
different types of **data structures**.  We have already seen an
example of a data structure: a Python list.  In general, data
structures:

1. provide a place to efficiently store and retrieve items of
information;

2. express some relationship between items; and

3. allow algorithms to find, reorder, or apply operations to items in
the data structure, using the implied relationship between the items.

Does the Python list provide a place to store and retrieve items?
Yes; for example, we might have a list of numbers, strings, or address
of `Body` objects.  What relationship can the list express between
items?  The items are in some *order* implied by the indices of items
in the list.  For example, we say that the fourth item follows the
third item in the list, and it precedes the fifth item.  We have seen
algorithms that reorder the list (changing the relationship between
items), find items in the list, or just apply operations to each item
in the list (for example, compute the acclerations and velocities of
all planets in a list of planets).

**Linked lists** are a type of data structure that you have not seen
yet in this course.  Unlike Python lists, linked lists are not built
in to Python, and we will have to implement them ourselves.

Linked lists provide many of the same features that Python lists do:

1. Linked lists contain an ordered sequence of items.
2. You can append, insert, or delete items from a linked list.
3. You can traverse a linked list, applying a function to each time in
the list in order.

For many purposes, Python lists are a good choice for containing
several items in sequential order.  Because Python lists are built in
to the language, they have been optimized for speed, and the time
costs for accessing, deleting, changing, or adding an item have small
constant factors associated with them.  Since we will write our own
linked lists, Python has to interpret the code we write, which will
take some time.  Nevertheless, for sufficiently large numbers of
items, linked lists may provide some advantages for some operations.

## Time costs of operations on Python lists

Let's look at how Python lists actually work.  Earlier in the course,
I expressed concern about how list operations are actually
implemented.  Here's how Python implements lists.  The items in a
Python list are stored in order in memory.  Consider a list of 4-byte
integers.  The 0th item in the list is stored at some address, let's
say 7080.  Then the first item would be stored at 7084.  The second
item would be at 7088, and so on.  The $i$th item would be at address
$7088 + 4i$.

### Runing time of inserting an item into a Python list

Let's say you have a Python list with 20 items.  You would like to
maintain the order of the items, and you would like to insert a new
item as the first item of the list.  You can do this with the `insert`
method of lists, like this:

~~~{.python}
l = [10, 14, 16, 22]
l.insert(0, 999)
~~~

The new item will be at index 0.  Notice that the item previously at
index 0 has to move to index 1, the item previously at index 1 has to
move to index 2, and so forth.  With some cleverness, you might
realize that the best way to do this operation would be to start at
the end of the list and move each item one position to the right (does
this idea remind you of insertion sort?), but no amount of cleverness
will save you from having to loop over each item in the list and
moving it.

Inserting into the beginning of the list is the worst case, since all
other items have to be moved to the right; inserting at the end of the
list is much cheaper.  So the worst case running time for insertion
into an $n$-item Python list is $O(n)$.

### Time cost of appending to a list: amortized running time

You might think that appending to the list (inserting at the end)
would have run time $O(1)$ since items do not have to be copied.  As
one of my favorite faux titles for a country song goes, "Purt near but
not plumb."  That is, it's *almost* true.  Here's why it is not
entirely true.  The list is stored in one sequential area of memory.
What happens if you try to make the list longer, and there is
something stored in memory just after the list?  In this case, Python
requests a new, larger amount of memory; the computer finds a new
location, and relocates the list to that new location by *copying* the
entire list.  If the list has $n$ items, then copying the list takes
$O(n)$ time, and so in the worst case (the situation where the list
has to be moved), appending to the list takes $O(n)$ time.  That is,
of course, a lot worse than the $O(1)$ time that you might have
expected.

But the designers of Python used a very clever idea to make sure that
this worst case running time for appending to a list occurs
infrequently.  We've seen that when Python appends to an existing list
and runs out of space, it has to request a new chunk of memory, where
it will move the list.  Let's say the list length is $n$.  But instead
of requesting enough space for a list of size $n + 1$, Python arranges
to have room for the new list to grow, and it requests space for a
list of size $2n$.  Overallocating in this way might seem as though it
wastes a lot of space&mdash;Python allocates up to twice the amount of
memory that the list really needs.  But the advantage is that if you
append items to the list over and over again, the list is moved
infrequently.

Let's say you add 128 items to a list in order, starting with a list
of size 1.  As you append items to the list, Python will increase the
memory allocated to the list.  At first, there is memory for 1 item.
Then for 2 items.  When that space fills up, Python allocates a list
with space for 4 items.  When that fills up, space for 8.  Then 16,
32, 64, and finally, 128.  Let's see what happens as we increase the
list size from 64 to 129.

Imagine that it costs 1 dollar to append an item to a list if Python
does *not* have to reallocate space and copy all the items, and that
it costs 1 dollar to copy an item when the list is copied to a new
location.  When we append an item to the list, let's charge not 1
dollar, but 3 dollars.  Of the 3 dollars, we'll use 1 dollar to pay
for the append operation, and we'll keep the other 2 dollars in a bank
account for the item; we'll spend this 2 dollars later.

Our list has 64 items, and now let's append another 64 items.  We're
charged $64 \times 3$ dollars for these append operations, with 64
dollars going toward paying for the appends and the remaining 128
dollars in the bank.  Now we add the 129th item.  Before appending
this item, we have to copy all 128 of the previous items into
reallocated space.  But we have 128 dollars already in the bank to pay
for it!  So we spend the 128 dollars to copy the 128 items, and then
we can append the 129th item, paying 1 dollar for the append operation
and banking the other 2 dollars.

The key idea here is that the list size doubles.  Each item pays 1
dollar for its own append, 1 dollar for itself to be copied one time,
and 1 dollar for a previously appended item to be copied one time.
Since we pay a constant amount, 3 dollars, for each item we can think
of the average time per append being constant, or $O(1)$.

We call this sort of analysis an **amortized running time** analysis,
because the possibly expensive cost of adding one item to the list is
offset by frequently adding items to the end of the list cheaply.  We
can "amortize" the cost over several append operations, and show that
the behavior of the append operation is pretty good over the long run.

### Time cost of deleting an item from a list

If you would like to maintain the order of the list and also delete an
item, then the worst-case cost for an $n$-item list is $O(n)$, since
all items following the deleted item have to be moved up by one
position in the list.  You can delete items from a list using the
special built-in operator `del` in Python:

~~~{.python}
del l[3]
~~~

Deleting from the end is a special case that is not expensive, since
no items in the list have to be moved; deleting from the end has time
cost $O(1)$.  We call deleting from the end **popping**, and there is
a special `pop` method on lists that removes from the end, returning
the item that was removed.

### Time cost of finding an item in a list

If the list is unsorted, then it takes $O(n)$ time to find the item in
an $n$-item list, by using linear search.  Similarly, finding the
smallest or largest item in an unsorted list takes $O(n)$ time.

### Tricks: Unordered lists (sets)

A list maintains items in order.  But sometimes you do not care about
the order of items in the list.  You can use this observation to
sometimes insert or remove items without occuring an $O(n)$ running
time.  If the order doesn't matter, just insert by appending.  When
you delete an item from the list, instead of shifting all the items
that follow the deleted item, you can fill the hole left by the
deleted item by moving in the item that's at the of the list.  In
Python, however, you should probably more properly use the built-in
`set` data structure to store items for which you do not care about
the order.  We will not discuss the `set` data structure further at
this time, but you are welcome to look it up in the Python
documentation.

## Linked lists

Linked lists, like Python lists, contain items in some order, but the
implementation of linked lists will be very different from that of
Python lists.  There are two reasons we will implement our own linked
lists:

1. Certain operations for linked lists are efficient in terms of time.
For example, inserting an item after another item in the middle of the
list costs $O(1)$ time&mdash;much better than the $O(n)$ time for
inserting into an $n$-item Python list.  For large enough linked
lists, in a situation where insertion in the middle is frequent, a
linked list might be the better choice, despite the smaller constant
factor for Python lists.

2. The approach we will use to create the linked list is an example
that we will build on to form more complex data structures, trees and
graphs, that express more interesting relationships between data than
the simple sequential list structure.

## Nodes in the list

Conceptually, we will create a linked list by making lots of objects
called **nodes**.  Each `Node` object will contain an item: the data
in the list.  The `Node` objects themselves could be anywhere in
memory, and will typically *not* be stored sequentially, or in any
other particular order, in memory.

How, then, can we tell which item in the list is first, which is
second, and what the order of items is?  Each `Node` object will
contain an instance variable that has the address of the next `Node`
object in the linked list.

Let's look at an example of how this might work.  (I should emphasize
that this is not yet a good implementation of a linked list.  I just
want to show you how nodes work.)  Here's a picture of a linked list
holding the names of three states, in order: Maine, Idaho, and Utah:

![](states-SLL.png)

The arrows indicate references.  The slash in the `next` instance
variable of the last `Node` object (the one whose data is `Utah`) is
how I indicate the value `None` in a picture.

Here's some simple code, in [node_example.py](node_example.py).
Although we usually put a class in its own file, I didn't put the
`Node` class in its own node.py file because I wanted to keep this
example short and in one place.

~~~{.python}
# node_example.py
# CS 1 class example by Devin Balkcom, modified by THC.
# Shows a simple way to use Node objects to form a linked list.

class Node:
    def __init__(self, data):
        self.data = data  # instance variable to store the data
        self.next = None  # instance variable with address of next node

# The head is the first node in a linked list.
head = Node("Maine")

# Create a new node.
another_node = Node("Idaho")

# Store the address of the Idaho node as the
#  next address of the first node in the list.
head.next = another_node

# Create a third node.
a_third_node = Node("Utah")

# Link the second node to the third node.
another_node.next = a_third_node

# An example of printing the data of the list in order:
node = head   # copy the address of the head node into node
while node != None:
    print node.data
    node = node.next
~~~

When we printed the list, we created a new, temporary variable `node`
that initially has the address of the first `Node` object in the
linked list.  We use that address to find the data to print (`Maine`),
and then we update `node` to hold the address of the next item.  We
print its data (`Idaho`) and update `node` again to hold the address
of the next item.  We print its data (`Utah`), and update `node` once
again.  Now, `node` equals `None`, and we drop out of the while-loop.

## Circular, doubly linked list with a sentinel

We did some things manually in the above example.  It would be nice to
have a class for a linked list that stored the address of the first
node, and that also provided methods for inserting into, deleting
from, appending to, and finding items in the list.

Our previous model for a linked list, in which only the address of the
next node is stored in any node, has some annoying special cases.  We
will therefore start with a slightly different implementation that has
a few convenient features that at first blush might seem more
complicated, but actually simplify the implementation of certain
methods, and even improve the running time of certain methods.

1. The list will be **doubly linked**: each node will contain a
reference to the *previous* node in the chain (in a `prev` instance
variable) as well as a reference to the *next* node.  Although this
structure makes it easy to iterate backward through the list, it makes
other, more common, operation easy, too.

2. The list will have a **sentinel** node, instead of a reference to a
first node.  This sentinel node is a special node that acts as the 0th
node in the linked list.  No data is stored in this special node; it's
there just as a placeholder in the list.

3. The list will be **circular**.  The last node containing data will
hold in its `next` instance variable the address of the sentinel node,
and the sentinel node's `prev` instance variable will hold the address
of the last node.

We call this structure a **circular, doubly linked list with a
sentinel**.  Quite a mouthful, indeed.  Practice saying it fast.

A circular, doubly linked list with a sentinel has the property that
*every* node references a next node and a previous node.  Always.
This uniform way of treating nodes turns out to be quite convenient,
because as we write methods to insert or delete nodes into the list,
we don't have to worry about special cases that would arise if the
last node didn't have a next node, and the first node didn't have a
previous node, but all interior nodes had both.

Here is a picture of a circular, doubly linked list with a sentinel
representing the same list as before:

![](states-DLL.png)

We create a class, `Sentinel_DLL`, to implement a circular, doubly
linked list with sentinel in [sentinel_DLL.py](sentinel_DLL.py):

~~~{.python}
# sentinel_DLL.py
# CS 1 class example for a circular, doubly linked list with a sentinel.
# Written by Devin Balkcom, modified by THC.

# Class for a node in a circular, doubly linked list with a sentinel.
class Node:
    def __init__(self, data):
        self.data = data  # instance variable to store the data
        self.next = None  # instance variable with address of next node
        self.prev = None  # instance variable with address of previous node

# Class for a circular, doubly linked list with a sentinel.
class Sentinel_DLL:
    # Create the sentinel node, which is before the first node
    # and after the last node.
    def __init__(self):        
        self.sentinel = Node(None)
        self.sentinel.next = self.sentinel
        self.sentinel.prev = self.sentinel
        
    # Return a reference to the first node in the list, if there is one.
    # If the list is empty, return None.
    def first_node(self):
        if self.sentinel.next == self.sentinel:
            return None
        else:
            return self.sentinel.next
    
    # Insert a new node with data after node x.
    def insert_after(self, x, data):
        y = Node(data)   # make a new Node object.

        # Fix up the links in the new node.
        y.prev = x
        y.next = x.next
        
        # The new node follows x.
        x.next = y
        
        # And it's the previous node of its next node.
        y.next.prev = y
    
    # Insert a new node at the end of the list.    
    def append(self, data):
        last_node = self.sentinel.prev
        self.insert_after(last_node, data)
    
    # Delete node x from the list.
    def delete(self, x):
        # Splice out node x by making its next and previous
        # reference each other.
        x.prev.next = x.next
        x.next.prev = x.prev
    
    # Find a node containing data, and return a reference to it.
    # If no node contains data, return None.
    def find(self, data):
        # Trick: Store a copy of the data in the sentinel,
        # so that the data is always found.  
        self.sentinel.data = data
        
        x = self.first_node()
        while x.data != data:
            x = x.next
            
        # Restore the sentinel's data.
        self.sentinel.data = None
        
        # Why did we drop out of the while-loop?
        # If we found the data in the sentinel, then it wasn't
        # anywhere else in the list.
        if x == self.sentinel:
            return None     # data wasn't really in the list
        else:
            return x        # we found it in x, in the list
        
    
    # Return the string representation of a circular, doubly linked
    # list with a sentinel, just as if it were a Python list.
    def __str__(self):
        s = "["

        x = self.sentinel.next
        while x != self.sentinel: # look at each node in the list
            if type(x.data) == str:
                s += "'"
            s += str(x.data)        # concatenate this node's data to the string
            if type(x.data) == str:
                s += "'"
            if x.next != self.sentinel:
                s += ", "   # if not the last node, add the comma and space
            x = x.next

        s += "]"        
        return s
~~~

### Creating an empty list:  `__init__` 

A `Sentinel_DLL` object has just one instance variable: `sentinel`.
This instance variable is a reference to a `Node` object.  I defined
the `Node` class in the sentinel_DLL.py file, rather than defining it
in its own file, because it's meant only to be part of a circular,
doubly linked list with a sentinel.

In order for the list to be circular, both the `next` and `prev`
instance variables of the sentinel node must contain the address of
the sentinel itself.  Here's what an empty list looks like:

![](empty-DLL.png)

It can at first be intimidating to see a line with as many dot
operators in it as

~~~{.python}
self.sentinel.next = self.sentinel
~~~

Just work left to right for each variable.  `self` is the address of a
`sentinel_DLL` object.  `self.sentinel` is the value of the `sentinel`
instance variable in the `sentinel_DLL` object.  We know that that
value contains the address of some `Node` object.  So
`self.sentinel.next` is the `next` instance variable of the sentinel
node in the `sentinel_DLL` object.  That's what we're assigning *to*.
The value we're assigning is `self.sentinel`, the address of the
sentinel node.

### Finding the first node

The `first_node` method just returns a reference to the first node in
the list, or `None` if the list contains just the sentinel.  The first
node is always the one after the sentinel.  If the node just after the
sentinel is the sentinel itself, then the list contains just the
sentinel, and the method returns `None`.  Otherwise, the method
returns a reference to the first node.

### Inserting a new node into a list

To insert a new node into the list, we need to know which node we're
inserting it after.  The parameter `x` is a reference to that node.
The parameter `data` gives the data that we're inserting.

First, the method creates a new `Node` object, referenced by `y`, to
hold the data that we want to add to the list.  Now we need to
"splice" the new node into the list, just as you can splice a new
section into a piece of rope or magnetic tape.  In order to splice in
the new node, we need to do a few things:

1.  Make the links in the new node refer to the previous and next
nodes in the list.

2.  Make the `next` link from the node that will precede the new node
refer to the new node.

3) Make the `prev` link from the node that will follow the new node
refer to the new node.

We have to be a little careful about the order in which we do these
operations.  Let's name the nodes for convenience.  We already have
`x` referencing the node that the new node, referenced by `y`, goes
after.  Let `z` reference the next node after `x` *before* we insert
`y`, so that `y` is supposed to go between `x` and `z`.  If we clobber
the value of the `next` instance variable of `x` too soon, we won't
have a way to get the address of `z`, which we will need.

It's good to be comfortable with the dot notation used so heavily in
the code, but it may help you to draw a picture.  Here's the same
method as above, but with a temporary variable introduced for `z`:

~~~{.python} 
    # Insert a new node with data after node x.
    def insert_after(self, x, data):
        y = Node(data)   # make a new Node object.
        z = x.next       # y goes between x and z

        # Fix up the links in the new node.
        y.prev = x
        y.next = z
        
        # The new node follows x.
        x.next = y
        
        # And it's the previous node of z.
        z.prev = y
~~~

### Iterating over a list

The `__str__` method shows an example of how to iterate over a list.
The first node in the list is the node after the sentinel.  We loop
over nodes, letting `x` reference each node in the list, until
returning to the sentinel again (recall that the list is circular).
To get to the next node in the list, we use the line `x = x.next`.

One other interesting thing about the `__str__` method is that I wrote
it to produce the same string as you'd see if the linked list were a
regular Python list.  When Python prints a list, it puts single quotes
around all strings in the list.  So I have a check to see whether
`x.data` is a string, and if it is, I add in the single quotes.

### Deleting from a list

The `delete` method takes a node `x` and deletes it from the list.
It's deceptively simple in how it splices the node out, by just making
its next and previous nodes reference each other.  Just we did for
insertion, let's rewrite the `delete` method, but with `y` referencing
`x`'s previous node and `z` referencing `x`'s next node:

~~~{.python}
    # Delete node x from the list.
    def delete(self, x):
        # Splice out node x by making its next and previous
        # reference each other.
        y = x.prev
        z = x.next
        y.next = z
        z.prev = y
~~~

### Searching a list

The `find` method searches the linked list for a node with a given
data value, returning a reference to the first node that has the
value, or `None` if no nodes have the value.  It uses linear search,
but in a linked list.

There's a cute trick that I've incorporated into my implementation of
`find`.  If you go back to the [linear search
code](../lecture13/linear_search.py) that we saw before, you'll notice
that each loop iteration makes two tests: one to check that we haven't
reached the end of the list and one to see whether the list item
matches what we're searching for:

~~~{.python}
def linear_search(the_list, key):
    index = 0
    while index < len(the_list):
        if key == the_list[index]:
            return index
        else:
            index += 1
        
    return None
~~~

Suppose that you *knew* you'd find the item in the list.  Then you
wouldn't have to check for reaching the end of the list, right?  So
let's put the value we're looking for into the linked list, but in a
special place that tells us if that's where we find it, then the only
reason we found it was because we looked everywhere else before
finding it in that special place.

Gosh, if only we had an extra node in the list that didn't contain any
data and was at the end of the list.  We do: the sentinel.  So we put
the value we're looking for in the sentinel's `data` instance
variable, start at the first node after the sentinel, and loop until
we see a match.  If the match was *not* in the sentinel, then we found
the value in the list.  If the match *was* in the sentinel, the the
value wasn't really in the list; we found it only because we got all
the way back to the sentinel.  In either case, we put `None` back into
the sentinel's `data` instance variable because, well, my mother told
me to.

### Appending to the list

The only other method in my `Sentinel_DLL` class is `append`.  Not
surprisingly, it finds the last node in the list and then calls
`insert_after` for that node.  How to find the last node in the list?
It's just the sentinel's previous node.

### Running times of the linked-list operations

What are the worst-case running times of the operations for a
circular, doubly linked list with a sentinel?  Let's assume that the
list has $n$ items ($n+1$ nodes, including the sentinel, but of course
the $+1$ won't matter when we use big-Oh notation).

The `__init__`, `first_node`, `insert_after`, `append`, and `delete`
methods each take $O(1)$ time, since they each look at only a constant
number of nodes.

The `__str__` method takes $O(n)$ time, since it has to visit every
node once.

The time taken by the `find` method depends on how far down the linked
list it has to go.  In the worst case, the value is not present, and
`find` has to examine every node once, for a worst-case running time
of $O(n)$.

### Testing the linked list operations

You can test the linked list operations by writing your own driver.  I
have a pretty minimal driver in
[test_sentinel_DLL.py](test_sentinel_DLL.py):

~~~{.python}
# test_sentinel_DLL.py
# CS 1 class example by THC.
# Tests the Sentinel_DLL class.

from sentinel_DLL import Sentinel_DLL
    
def test_sentinel_DLL():
    # Make a linked list with Maine, Idaho, and Utah.
    l = Sentinel_DLL()
    l.append("Maine")
    l.append("Idaho")
    l.append("Utah")
    
    # Add Ohio after Idaho.
    node = l.find("Idaho")
    if node != None:
        print node.data
        l.insert_after(node, "Ohio")
    print l
    
    # Delete Idaho.
    if node != None:
        l.delete(node)
    print l
    
    # Empty out the list, one node at a time.
    while l.first_node() != None:
        l.delete(l.first_node())
        
    print l
    
test_sentinel_DLL()
~~~
