# Recursion, continued

## Fibonacci numbers

Last time, we started looking at how to compute Fibonacci numbers.

The *n*th Fibonacci number, fib(*n*), is defined as follows:

* fib(1) = fib(2) = 1, and
* fib(*n*) = fib(*n*&ndash;1) + fib(*n*&ndash;2), if *n* > 2. 

Here's a table with the first eight Fibonacci numbers:

*n*       1   2   3   4   5   6   7   8
-------- --- --- --- --- --- --- --- ---
fib(*n*)  1   1   2   3   5   8  13  21

Expressed recursively:

* **Base cases**: *n* = 1 and *n* = 2.
* **Recursive case**: We know how to compute fib(*n*&ndash;1) and
fib(*n*&ndash;2).  Use them to compute fib(*n*).

[fib_recursive.py](fib_recursive.py) recursively computes Fibonacci
numbers:

~~~{.python}
# fib_recursive.py
# Recursively computes the nth Fibonacci number.

def fib(n):    
    if n == 1 or n == 2:
        return 1    # base case
    else:    
        return fib(n - 1) + fib(n - 2)
    
print fib(7)
~~~

This program uses two recursive calls.  We need to keep track of
exactly where each call was made.  Are we returning to the first or to
the second recursive call?  Keeping track of where we are in the
program is hard for people to do without making mistakes, but
computers are quite good at it.  They don't get distracted.

A helpful way to visualize what happens in recursive programs is a
**recursion tree**.  A recursion tree has **nodes** connected by
**branches**, or **links**.  Here's a recursion tree for computing
`fib(3)`:

![](small-fib-rec-tree.png)

The way to interpret a recursion tree like this one is that we label
each node with the value of *n* at a call, and each link denotes a
call.  In this case, the call for *n* = 3 ends up making two calls,
one for *n* = 2 and one for *n* = 1.

For our `fib` method, whenever the call is for a value *n* that is
greater than 2, `fib` will make two recursive calls, one for
*n*&ndash;1 and one for *n*&ndash;2.

For *n* = 6, for example, we get the following recursion tree:

![](big-fib-rec-tree.png)

In computer science, we draw trees with the **root** at the top; for
this tree, the root is labeled 6.  The nodes on the bottom are **leaf
nodes**, or **leaves**, because if we drew the tree with the root at
the bottom, they would look like leaves of the tree.

Notice that we repeat a lot of work.  For example, to compute
`fib(6)`, we call both `fib(5)` and `fib(4)`.  But we also call
`fib(4)` in computing `fib(5)`.  Each call to `fib(4)` will return the
same result.  Yet we end up making separate calls to `fib(4)`,
repeating the work each time.

Notice also that there are 8 leaf nodes.  Each leaf node corresponds
to a call that is a base case.  Each base case returns 1, thereby
contributing 1 to the eventual result.  So it should be no surprise
that there are 8 leaf nodes, when the call `fib(6)` returns 8.

If fib(*n*) equals *x*, then this recursive program ends up in the
base case *x* separate times.  In other words, it sums up the value 1
a total of *x* times.  The problem is that the value of fib(*n*) is
exponential in *n*.  (In particular, it is $((1 + \sqrt 5 ) / 2)^n /
\sqrt 5$, rounded to the nearest integer.)  So, the number of times we
get down to a base case is exponential in *n*, meaning that it gets
very large very quickly.

This recursive Fibonacci program is easy to write, but there are far
more efficient ways to compute Fibonacci numbers.  (In fact, I just
told you one: compute the value of a particular expression and round
it.)

## Graphics and recursion

If we want to draw something that is similar to itself, recursion can
be a good way to go about it.  A branch of a tree looks a bit like a
little tree, for example, and a piece of a snowflake may itself look
like a little snowflake.

Here are a couple of fun examples.  Here's the code that draws the
Sierpinsky Gasket from before, in [sierpinsky.py](sierpinsky.py):

~~~{.python}
# sierpinsky.py
# Draws a Sierpinksy Gasket.

from cs1lib import *

# Draw a Sierpinsky Gasket of width and height d,
# with upper left at (x, y).
def draw_sierpinsky(x, y, d):
    if d <= 1:
        draw_point(x, y)
    else:
        draw_sierpinsky(x + d/2, y, d/2)
        draw_sierpinsky(x + d/2, y + d/2, d/2)    
        draw_sierpinsky(x, y, d/2)

def main():
    set_clear_color(0, 0, 0)
    clear()
    set_stroke_color(1, 1, 0)
    
    draw_sierpinsky(0, 0, 400)
    
start_graphics(main)
~~~

The second is a wheel-like object drawn with circles and lines, in
[wheel.py](wheel.py).  Wheels within wheels, and animated to boot!  I
won't go through this code in class, but it's cool to watch the
result.

~~~{.python}
# wheel.py
# cs1 class example
#  based on a cs2 example by Fabio Pellacini
#  python version October, 2011, Devin Balkcom
#  Animation by THC.

from cs1lib import *
from math import pi, sin, cos

OUTER_RADIUS = .37  # the relative radius of circles around the outer rim of the wheel
                    # a scaling of .37 causes the circles to just barely touch
                    
OUTER_DISTANCE = 1 - OUTER_RADIUS

FRAME_RATE = 30
TIMESTEP = 1.0 / FRAME_RATE

WINDOW_WIDTH = 400
WINDOW_HEIGHT = 400

def radians(degrees):
    return degrees * pi / 180.0

# Compute the x coordinate of a location that is
#  a distance 'distance' from a point cx, cy, with angle
#  'angle' from the horizontal

def compute_polar_x(cx, angle, distance):
    return cx + cos(radians(angle)) * distance
    
def compute_polar_y(cy, angle, distance):
    return cy + sin(radians(angle)) * distance    
    
# Recursively draw the wheels.
def draw_wheel(x, y, r, angle):
    # Draw this wheel.
    set_fill_color(0, 0, 1, .2)
    draw_circle(x, y, r)
    set_fill_color(0, 1, 0, .2)
    draw_circle(x, y, r * (OUTER_DISTANCE - OUTER_RADIUS)) 

    # If this wheel is small enough, we're done.
    if r < 20:
        return

    # Not too small.  Draw the five outer wheels
    for i in range(5):
        new_x = compute_polar_x(x, angle, r * OUTER_DISTANCE)
        new_y = compute_polar_y(y, angle, r * OUTER_DISTANCE)
        
        angle += 72
    
        draw_wheel(new_x, new_y, r * OUTER_RADIUS, angle)
        draw_line(x, y, new_x, new_y)  # draw the spokes connecting to the outer wheels
    
def main():
    enable_smoothing()
    
    set_clear_color(1, 1, 1)
    clear()
    
    starting_angle = 0
    
    while not window_closed():
        clear()
        draw_wheel(WINDOW_WIDTH / 2, WINDOW_WIDTH / 2, 
                   WINDOW_HEIGHT / 2, starting_angle)
        starting_angle += 1
        request_redraw()
        sleep(TIMESTEP)

start_graphics(main, "The Wheel", WINDOW_WIDTH, WINDOW_HEIGHT)
~~~

## The Towers of Hanoi

Our next example of recursion solves a puzzle called *The Towers of
Hanoi*.  In this problem, we have to find a sequence of actions that
lead toward a goal.  You might have heard of a generic name for
problems of this type: *artificial intelligence.* Certain situations
lend themselves to good AI solutions.  For example, the top AI chess
programs consistently beat the best human grandmasters.

The Towers of Hanoi puzzle has three pegs, numbered 1, 2, and 3.  On
peg 1 are *n* disks, in increasing order by size, with the smallest
disk (disk 1) on top and the largest disk (disk *n*) on the
bottom. The object is to move all disks from peg 1 to peg 2, obeying
two rules:

1. Only one disk may be moved at a time.  The other disks must be
resting on one of the three pegs.  These rules imply that only the top
disk on a peg may be moved.

2) No disk may ever rest on a smaller disk.

The Towers of Hanoi puzzle is being solved, even now, by monks in the
high, inaccessible reaches of Tibet.  They use 64 disks, and they
believe that when they have completed moving all 64 disks from peg 1
to peg 2, the world will come to an end.

We solve this problem recursively in [hanoi.py](hanoi.py):

~~~{.python}
# Hanoi.py
# Originally by Scot Drysdale
#  Translated into Python by Devin Balkcom.
#  Minor changes by THC.
#  Solves the Towers of Hanoi puzzle.

def move_disk(disk_number, from_peg, to_peg):
    print "Move disk " + str(disk_number) + " from peg " \
        + str(from_peg) + " to peg " + str(to_peg) + "."    

def solve_hanoi(n, start_peg, end_peg):
    if n == 1:
        # Base case: Move a "tower" of height 1 by simply moving
        # the only disk.
        move_disk(n, start_peg, end_peg)
    else:   # recursive case
        # A trick to compute the number of the spare peg:  1 + 2 + 3 = 6.
        spare_peg = 6 - start_peg - end_peg  
        
        # Move all but the bottom disk from start peg to spare peg.
        solve_hanoi(n - 1, start_peg, spare_peg) 
    
        # Move the bottom disk from the start peg to the end peg.
        move_disk(n, start_peg, end_peg)
        
        # Move all but the bottom disk from spare peg to end peg.
        solve_hanoi(n - 1, spare_peg, end_peg)
    
solve_hanoi(5, 1, 2)
~~~

The recursive function to solve the problem has the header

~~~{.python}
solve_hanoi(n, start_peg, end_peg)
~~~

This function moves all disks numbered 1 through *n* from peg
`start_peg` to peg `end_peg`.  It uses the remaining peg as a spare.

How do you solve this problem?  Let's start with a really easy case:
*n* equals 1.  There is only disk 1, and we can just move it from peg
1 to peg 2.  Notice that there is nothing special about pegs 1 and 2.
We could just as easily move disk 1 from peg 1 to peg 3, or from peg 3
to peg 2, etc.

Now let's look at what happens when *n* equals 2.  We move disk 1 from
peg 1 to the spare peg, which is peg 3.  Now disk 2 is exposed, and we
can move it from peg 1 to peg 2.  All that remains is to move disk 1
from peg 3 to peg 2.  Again, there is nothing special about pegs 1 and
2.  We could just as easily have moved disks 1 and 2 from peg 1 to peg
3 (move disk 1 from peg 1 to peg 2, move disk 2 from peg 1 to peg 3,
move disk 1 from peg 2 to peg 3), or from peg 3 to peg 2 (move disk 1
from peg 3 to peg 1, move disk 2 from peg 3 to peg 2, move disk 1 from
peg 1 to peg 3), etc.

How about when *n* equals 3?  We would like to get disk 3 exposed on
peg 1 and have no disks on peg 2.  In other words, we would like to
move disks 1 and 2 from peg 1 to peg 3.  But that's a subproblem that
we just said we know how to solve!  So we do it, by calling `hanoi(2,
1, 3)`.  Now we can move disk 3 from peg 1 to peg 2.  It remains to
move disks 1 and 2 from peg 3 to peg 2.  Again, we know how to solve
this subproblem: call `hanoi(2, 3, 2)`.

In general:

* **Base case:** *n* equals 1.  We just move disk 1 from `startPeg` to
    `endPeg`.

* **Recursive case:** *n* is at least 2.  Assume that we already know
    how to move disks 1 through *n*&ndash;1 from any peg to any other peg.

    1.  Determine which peg is the spare peg.  Trick: We know that the
    pegs are numbered 1, 2, and 3.  Since we know the numbers of two
    of them, then the remaining peg must be the sum 1+2+3 (by advanced
    mathematics, you can show that this sum is 6), minus the two pegs
    whose numbers we know.
    
    2. Solve the subproblem of moving disks 1 through *n*&ndash;1 from
    the start peg to the spare peg.  Now the end peg is empty, and the
    start peg holds only disk *n*.

    3. Move disk *n* from the start peg to the end peg.

    4. Solve the subproblem of moving disks 1 through *n*&ndash;1 from
    the spare peg to the end peg.

### Analysis of Towers of Hanoi

We can draw a recursion tree for the Towers of Hanoi program.  Here's
a recursion tree for the call `solve_hanoi(3, 1, 2)`:

![](hanoi_tree.png)

Each node is labeled by the three parameters of the call.  A node that
is underlined represents printed output.  For example, the node
labeled <u>2,1,3</u> represents the printed output "Move disk 2 from
peg 1 to peg 3."  If we read the leaves of this tree from left to
right, then the order in which we read then underlined nodes
corresponds to the printed output from the program.

Recall that Tibetan monks are playing a version of Towers of Hanoi
with 64 disks, and when they finish, the world will end.  Is the world
about to end?  Should we be going wild in the streets?

No.  It will take 2^64^ &ndash; 1 moves before the world ends.
(Remember that 2^64^ is a really, really big number.)  Even if the
monks could move one disk per second, the sun will have burned out
long, long before they finish.

### Bonus coverage: Proving that 2^64^ &ndash; 1 moves are needed

I won't hold you responsible for this material, but you might be
interested to see how we can *prove* mathematically that the
`solve_hanoi` function makes 2^64^ &ndash; 1 moves when it is given an
instance with 64 disks.

We need a little mathematical notation. Let's define $T(n)$ to be the
number of moves we need to make with *n* disks.  With just one disk,
we need to make only one move, and so $T(1) = 1$.  With two disks, we
make three moves, and so $T(2) = 3$.  In general, with *n* disks, we
need to solve a subproblem with *n*&ndash;1 disks, requiring $T(n-1)$
moves; then move disk *n*, requiring one move; and then solve another
subproblem with *n*&ndash;1 disks, requiring another $T(n-1)$ moves.
In other words, we can write $T(n) = T(n-1) + 1 + T(n-1)$, which we
simplify to $T(n) = 2T(n-1) + 1$.  This way of writing a function such
as $T(n)$ in terms of the same function on smaller values, such as
$T(n-1)$, is known as a **recurrence equation**.  Several mathematical
techniques apply to solving recurrence equations.  We'll use
mathematical induction to prove a solution for this one.

In **mathematical induction**, we are trying to prove that some
statement is true for all integers that are greater than or equal to
some value.  We first show that the statement is true for the **base
case** (the "some value").  Then we show the **inductive step**: that
if it is true for all integers less than *k*, it's true for *k* as
well.  For example, suppose that the base case is 1.  We show that our
statement is true for 1.  Now suppose that we've also shown the
inductive step.  Since the statement is true for 1, then it's also
true for 2.  And since it's true for 2, it's also true for 3.  And
since it's true for 3, it's also true for 4.  We can continue this
line of reasoning infinitely, and so we have shown that the statement
is true for all integers greater than or equal to the base case.

Let's use mathematical induction to prove that in our Towers of Hanoi
problem, $T(n) = 2^n - 1$.

In the base case, we let $n = 1$. Then it's easy to see that $T(1) = 1
= 2^1 - 1$.  Thus, our hypothesis that $T(n) = 2^n - 1$ holds for the
base case.

Now we show the inductive step.  We start with an inductive hypothesis
that says our property is true for $n - 1$, and we want to prove that
if our property is true for $n - 1$, then it's also true for $n$.
Here, our inductive hypothesis is that $T(n - 1) = 2^{n-1} - 1$.  We
want to show that $T(n) = 2^n - 1$.  Using our formula for $T(n)$, we
have

$T(n) = 2T(n - 1) + 1$ \
$= 2 (2^{n-1} - 1) + 1$ \
$= 2 (2^{n-1}) - 2 + 1$ \
$= 2^n - 1$

And that is precisely what we needed to show!  When $n = 64$, the
Tibetan monks will have to make $2^64 - 1$ moves.

# Analyzing algorithms

Computer science is more than about just programming.  It's also about
understanding characteristics of the problems we study and comparing
algorithms for solving problems.  Whatever the problem we study, we
will be concerned with two aspects when we analyze an algorithm:

* **Correctness**: Showing that our algorithms produce the correct
    answer.

* **Efficiency**: Understanding how efficient our algorithms are, in
    terms of resources used.  The resource that we most often analyze
    is *time*.  (After all, time is money, right?)

## Linear search vs. binary search

One of the most basic problems in computer science is finding the
index of an item in a list.  Let's assume the list is sorted, so that
we can use the technique of binary search on the list if we want to.
Maybe it's a list of country names, sorted alphabetically.

~~~{.python}
countries = ["Afghanistan", "Albania", "Algeria", "Andorra", "Angola",
  "Anguila", "Argentina", "Armenia", "Aruba", ..., "Zimbabwe"]
~~~

(The ... means that I got tired of typing country names, but let's
assume that every country name is in the list.)

Let's say you'd like to find the index of a country in the list.
(Perhaps "Yemen" or "Afghanistan".)  I can think of at least three
approaches.

* Approach #1: **Random search.** Pick a random index.  Check whether
  that location contains the item you were looking for.  If not,
  repeat.

* Approach #2: **Linear search.** Loop over the items in the list,
  starting at the index at index 0, then the item at index 1, index 2,
  and so on until you find the country name you are looking for, or
  you have exhausted the entire list without finding it.

* Approach #3: **Binary search.** (Possible only because the list is
  sorted.)  You did this in [Short Assignment
  8](../../shortassign/binarysearch/sa_binarysearch.html).  Maintain
  indices into a sublist under consideration.  If the sublist is ever
  empty, then the country name is not found.  Otherwise, divide the
  current sublist in half.  Check whether the item at the midpoint is
  what we're looking for.  If so, return that index.  Otherwise, check
  to see whether the midpoint item is greater than what we're looking
  for or less than what we're looking for.  Based on the results of
  the comparison, discard either the first or second half the sublist,
  updating the indices demarcating the sublist appropriately, and
  repeat with the new sublist.

### Worst-case and best-case analysis

Which of these methods will find the item we are looking for most
quickly?  It depends on the input we are given and perhaps on other
factors.  Let's consider the best and worst possible outcomes for each
of our approaches.

* **Random search.** In the best case, we get lucky, and the item is
  the first one we look at.  The time taken would be however long it
  takes for Python to examine a single item in the list.  In the worst
  case, however, random numbers are picked forever, and the item is
  never found.  (As time approaches forever, the probability of this
  unfortunate outcome occuring approaches 0&mdash;but we will not do
  probabalistic analysis of running time in this class.)

* **Linear search.** In the best case, the item we are looking for
  happens to be the first item of the list, just like the best case
  for random search.  In the worst case, the item we want is at the
  end of the list.  If the list contains $n$ items, Python will have
  to examine $n$ items.

* **Binary search.** In the best case, the item we are looking for is
  at the midpoint of the entire list, and the item is found in the
  first place we look, just like the best cases of random search and
  linear search.  In the worst case, the algorithm has to divide the
  list in half over and over again until the remaining sublist is
  empty.  How many times can a list of length $n$ be divided in half
  before the sublist is empty?  The answer turns out to be one more
  than $\log_2 n$, the "base-2" logarithm of $n$.  This number is much
  less than $n$, for large $n$.

The worst-case running time is often of more interest than best-case
running time, since it is nice to be able to guarantee the performance
of software.  If we can say "the worst case running time for this
algorithm is OK, as long as the list is not too long," that's good
information.  It's often less useful to say "this algorithm might be
really fast, if you give it just exactly the right input and the stars
align."

Sometimes, the worst-case and best-case running times are the same.
For example, if I ask you to find the smallest item in an *unsorted*
list of length $n$, the obvious (and best) algorithm of "look at every
every item in the list, always keeping track of the smallest seen so
far" will require looking at all $n$ items.  Even if the smallest item
happens to be located in the first spot, you won't *know* it's the
smallest until you've looked at every item.  The best and worst cases
behave exactly the same.

## The worst-case running time often depends on the size of the input

How fast an algorithm runs often depends on how much data there is.
(This is not always true.  For example, what if the problem were to
simply return a randomly selected item from a list?  Or if the problem
were to find the smallest item in a *sorted* list?)

## Review of logarithms

The binary-search method above is an example of a repeated halving
approach to solving a problem.  We divide the list (or phone book, if
you recall from our first lecture) in half.  We consider each half of
the original list and solve the problem for that sublist.  One of the
sublists is trivial to solve the problem for: the item cannot be in
that sublist, and so we eliminate that entire sublist from further
consideration.  The item *might* be in the other sublist.  We divide
that sublist in half and solve the two smaller problems.  We repeat
this process until either we find what we're looking for or the
sublist under consideration is empty.

The worst-case running time depends on how many times we can divide
the list in half before we get down to an empty sublist.  The answer
turns out to be the base-2 logarithm of the size of the original list.
Let's review logarithms in more detail.  If you're loga-phobic I would
like you to be comfortable with them.

Suppose we take a number, say $n$, and we halve it, getting $n/2$.  We
halve it again, getting $n/4$.  And again, getting $n/8$.  We continue
halving, until we get down to a number that is less than 1.  Since we
cannot have a sublist with, say, half an item, any sublist size that
is less than 1 must be 0.

Now, in order for the sublist size to become 0, it must have been 1
the previous time.  Let's answer the following question:

> How many times do we halve $n$ until we get down to 1 (or less)?

The answer turns out to be the base-2 logarithm of n, which we write
as $\log_2 n$.

Let's see what base-2 logarithms are, and why $\log_2 n$ turns out to
be the answer to our repeated-halving problem.

Let's try the opposite operation.  Let's start with $1$ and keep
doubling until we get up to $n$.  Since doubling is the inverse
operation of halving, the number of times we have to double, starting
at $1$, until we get to $n$, is equal to the number of times we have
to halve, starting at $n$, until we get to $1$.

Let's make a table:

 doublings                result 
----------------------   -------------------
   0                                1
   1 	                            2   
   2 	                            4
   3 	                            8
   4                               16
   5                               32
   6                               64

It's pretty easy to see the pattern here. Each time, we're multiplying
by another factor of 2.  After doubling $x$ times, we have the number
$2^x$.

The answer to our question of how many times we have to double,
starting at 1, until we get to $n$ is the same as the answer to the
following question: for what value of $x$ is $2^x = n$?

That's exactly what $\log_2 n$ is! It's the exponent $x$, that we
raise $2$ to, in order to get $n$.  Put precisely:

> $2^x = n$ if and only if $\log_2 n = x$.

So, we see that if we start at 1, and we double $\log_2 n$ times, we
get $n$.  Therefore, if we start at $n$, and we halve $\log_2 n$
times, we get 1.  So, the answer to the question, "How many times do
we halve $n$ until we get down to 1?" is $\log_2 n$ times.

The above discussion assumes that $n$ is a power of 2.  That won't
always be the case, and we can handle it with some math that we don't
cover in CS 1.  We also haven't dealt with the case in which we have
to get the sublist size down to 0 (recall: that's the worst case for
binary search, because what we're looking for is not found).  But
that's not much of a problem, because once the sublist size gets down
to 1, the next sublist size will be 0.  So the number of halvings to
get down to 0 is one more than the number of halvings to get down to
1.

Fortunately, the notation that we will see next time for
characterizing running times allows us to ignore such inconvenient
issues.

When computer scientists use logarithms, they almost always use base-2
logs.

## Analyzing the running time for a program

So far, we've only talked loosely about the running time of
algorithms.  We can see that binary search, given the worst possible
input for a binary search, will require fewer "steps" to complete than
a linear search, given the worst possible input for linear search.
Let's try to do a more precise analysis, given some code that
implements an algorithm.

Here is my implementation of linear search, from
[linear_search.py](linear_search.py).  (I have removed comments to
save space.  Of course, your own implementation would have comments,
rrrright?)

~~~{.python}
def linear_search(the_list, key):
    index = 0
    while index < len(the_list):
        if key == the_list[index]:
            return index
        else:
            index += 1
        
    return None
~~~

Let's say that `the_list` has the length $n$.  Let's analyze how much
time the function will take to run in terms of $n$.

First, Python has to create a local variable `index`.  That's pretty
fast, and it's done only once, but let's measure that time anyway, and
call it $c_1$, where $c_1$ is some constant that does not depend in
any way on the length $n$ of `the_list`.

Now let's look at the while-loop.  In a worst-case analysis, we have
to assume that the item is not in the list at all.  The body of the
while-loop will execute $n$ times.  It seems reasonable to assume that
each execution of the body will take a constant amount of time (or at
least an amount upper-bounded by a constant); let's call it $c_2$.  So
the total time spent executing the body of the while-loop, in the
worst case, is $c_2 n$.

But we're not quite done.  In the worst case, the last test in the
while-loop header comes up `False`, and it's when `index` equals
`len(the_list)`.  That's the $(n+1)$st time we make that test (because
we've already made the tests for `index` equaling $0, 1, 2, \ldots,
n-1$).  We also have to return `None` in the worst case.  That last
test and returning `None`, together, take some constant amount of
time; let's call it $c_3$ (again, independent of $n$).

Now, as they say in a French restaurant, "L'addition s'il vous plait."
("The check, please.")  The running time of linear search in the worst
case is therefore no worse than

$c_1 + c_2 n + c_3$ ,

or

$c_2 n + (c_1 + c_3)$ .

The exact values of $c_1$, $c_2$, and $c_3$ depend on the speed of the
computer the program is run on and the efficiency of the Python
implementation.

In class, we'll look at my implementation of binary search from
Short Assignment 8.

If we look at the time for each recursive call on its own, not
counting the time for the recursive calls that it makes, we see that
each recursive call takes a constant amount of time.  We check whether
we have an empty sublist, compute the midpoint, check whether the key
is in the position given by the midpoint, and recursively call the
function on the proper half.  Each of these steps, *within a given
recursive call* takes constant time; let's call it $c_4$.  And, as
we've seen, in the worst case, the number of recursive calls is at
most $\log_2 n + 1$.  So the total worst-case time for binary search
is

$c_4 (\log_2 n + 1)$ ,

or

$c_4 \log_2 n + c_4$ .

## Comparing linear search and binary search

So which is faster, linear search or binary search?  It depends: on
the suitability of the input we get for the particular algorithm
(linear search will find "Afganistan" faster than binary search, for
the example list), on the length of the list, and on the size of the
constants.  In the worst case, the question is, when is

$c_4 \log_2 n + c_4$

smaller than 

$c_2 n + (c_1 + c_3)$ ?

I claim that whatever the value of the constants, for large enough
$n$, binary search will be faster, if you assume the worst-case input
for each algorithm.  Let's say that linear search is run on a HAL 9000
computer that is blazingly fast and ultra-modern, and that binary
search is run on a vintage Apple II.  Let's assume that the HAL 9000
executes each iteration of the while-loop in linear search in 0.00001
seconds, so that $c_2 = \mbox{0.00001}$, and let's say that $c_1 + c_3
= 0$.

Now let's take the Apple II.  It is much slower, and it takes 0.01
seconds to execute each recursive call of binary search, so that $c_4
= \mbox{0.01}$.

How large does $n$ have to be before the Apple II wins?  I claim that
$\log_2 16384 = 14$; you can check for yourself that $2^14 = 16384$.
So, when $n = 16384$, the Apple II takes $\mbox{0.01} \times 14 +
\mbox{0.01} = \mbox{0.14} + \mbox{0.01} = \mbox{0.15}$ seconds.  For
the same input size, the HAL 9000 takes $\mbox{0.00001} \times 16384 =
\mbox{0.16384}$ seconds.  The Apple II wins!

Perhaps you're thinking that I rigged the game here by choosing
constants $c_2$ and $c_4$ so that the Apple II would win.  (The HAL
9000 would not permit me to do that.)  Let's just choose a longer
list.  Consider a list of length 1 million.  Since $\log_2 10^6$ is
about 20, the Apple II with binary search will take about $\mbox{0.2}$
seconds for the search.  The HAL 9000 will take $10$ seconds.  There's
no way that the difference between $c_2$ and $c_4$ will overcome the
Apple II's $9.8$-second lead.  From now on, we will ignore these
leading constant coefficients when comparing algorithms, since once
$n$ gets large enough, the constants become less important than how
the running time varies with $n$.

## Orders of growth and big-Oh notation

We say that **in the worst case, linear search takes linear time in
$n$, the size of the input list** because the running time scales
linearly with $n$.  If the length of the input list doubles, the
running time of linear search doubles, in the worst case.  If the size
of the input list quadruples, the running time quadruples, in the
worst case.

We say that **in the worst case, binary search takes logarithmic time
in $n$, the size of the input list** because the running time scales
like a $\log_2$ function.  If the original size of the list doubles
from $n$ to $2n$, the running time increases from $c_4 \log_2 n + c_4$
to $c_4 \log_2 (2n) + c_4$.  How much bigger is this running time?
Forget about the additive $c_4$ term; it's insignificant.  We can
analyze $c_4 \log_2 n$ vs. $c_4 \log_2 (2n)$ either by thinking about
the algorithm or with some equations.  Thinking about the algorithm,
doubling the length of the list will require that binary search
recurse just one more time.  That's not very expensive; it just costs
$c_4$ more time, even though we *doubled* the size of the list.  If
you prefer to use equations, $c_4 \log_2 (2n) = c_4 \log_2 n + c_4
\log_2 2 = c_4 \log_2 n + c_4$.

We can see that in general, algorithms that take logarithmic time are
always faster than algorithms that take linear time, at least once $n$
is large enough.  As $n$ gets even bigger, the algorithm with
logarithmic time will win by even more.  The function $\log_2 n$ grows
much more slowly than the function $n$.

We are usually only concerned with running time of an algorithm once
its input size $n$ gets large.  For large enough $n$, a logarithmic
running time is better than a linear running time, *regardless of the
constants*.  Computer scientists use a standard notation to indicate
that the running time grows "like" some function, ignoring the
constants.  If the running time is linear (grows linearly with $n$) in
the worst case, we say that the running time is $O(n)$, pronounced
"big-Oh of $n$".  If the running time is logarithmic (grows with the
logarithm of $n$), we say that the running time is $O(\log_2 n)$.

Big-oh notation indicates that for large enough $n$, ignoring
constants, the running time is no worse than the function in the
parentheses.  I can therefore say that binary search's running time is
$O(\log_2 n)$ and that linear search's running time is $O(n)$.

In fact, we can drop the base of the logarithm when we use big-Oh
notation.  Why?  Because of this mathematical fact: for any constants
$a$ and $b$,

$\displaystyle \log_a n = \frac{\log_b n}{\log_b a}$ .

So the difference between taking the logarithm of $n$ using base $a$
vs. base $b$ is just the constant factor $\log_b a$, and we've already
decided that constant factors don't matter in big-Oh notation.

I slipped something in before, when I said that "Big-oh notation
indicates that for large enough $n$, ignoring constants, the running
time *is no worse than* the function in the parentheses."  Big-Oh
notation gives us an **upper bound** on the running time's rate of
growth, but it doesn't say *anything* about how low the running time's
rate of growth might be.  If I tell you that I have an amount of money
in my wallet, and I guarantee that this amount is at most a thousand
dollars, I might have a thousand dollars, or I might have only ten
cents.

So if an algorithm's running time is $O(n)$, it might actually take
$cn$ time for some constant $c$, but it might be even faster.  For
example, it might take only $c\: \log_2 n$ time.  In other words, any
running time that is $O(\log\: n)$ is also $O(n)$: it's at least as good
as linear, if not better.  Put another way: it is true that *binary*
search runs in $O(n)$ time, but that is not the most accurate
statement you can make; it would be more accurate to say that binary
search runs in $O(\log\: n)$ time.

## Other orders of growth: Constant and quadratic orders of growth

Are there algorithms that are $O(n^2)$?  Sure.  Here's one:

~~~{.python}
def do_something(n):
    for i in range(n):
        for j in range(n):
            print i, j        
~~~

Let's assume that the print statement takes some constant amount of
time to execute, $c$ seconds.  How many times will that statement (the
body of the innermost loop) be executed?  It looks like $n^2$ times.
So the runtime is roughly $c n^2$.  Ignoring the constants, we say
that the function has a running time of $O(n^2)$.

What if an algorithm takes constant time, regardless of the input?

~~~{.python}
def do_something_else(n):
    print "My name is Inigo Montoya"
~~~

We say in this case that the function is $O(1)$, since $1$ is what we
get for the running time if we drop the constants.

## Ranking algorithms by running time

It's true that not all $O(n^2)$ algorithms take the same time.  The
constants might be different, and the best-case performance may be
different for different algorithms.  However, it is true that in the
worst case, for large enough $n$, any $O(n^2)$-time algorithm runs
faster than any algorithm for which the runtime is any positive
constant times $n^3$.  We therefore can think of all $O(n^2)$
algorithms as being roughly in the same family.  We can do a ranking
by running time:

$O(1) < O(\log\: n) < O(\sqrt n) < O(n) < O(n \log\: n) < O(n^2) <
O(n^3) < O(2^n) < O(n!)$

An algorithm whose rate of growth depends on the factorial of the
input $n$ is going to run for a very long time indeed, for large
values of $n$.

Sometimes, you will see a running time function that looks like this:

$c_1 n^2 + c_2 n + c_3$.

In this case, we can drop all of the lower-order terms and say that
the function is $O(n^2)$.  Why?  For large enough $n$, $n^2$ grows
much more quickly than $n$.  For large $n$, another function $c_4 n^2$
would be larger than $c_1 n^2 + c_2 n + c_3$, if $c_4$ is even a
little bit larger than $c_1$.  Since $c_4 n^2$ is $O(n^2)$, $c_1 n^2 +
c_2 n + c_3$ must be, too.

The take home message: compute the running time for a function, using
constants if you need to.  Then drop the constants and just take the
term in the expression that grows the fastest.  The running time is
big-Oh of that term.

## What is this "$n$" anyway?

The value $n$ represents some characteristic of the input that
determines the running time.  Maybe $n$ is the length of a list that's
input to the function, or maybe $n$ is the value of some parameter
that is input to the function.  We use $n$ instead of saying
specifically what the term is so that we can discuss the rate of
growth of the running time of an algorithm in more general terms.
However, we should always know specifically to what $n$ refers before
discussing the running time.

## Exercises and exams

From now on, given an algorithm or piece of code, you should be able
to

1. Identify what quantity $n$ the running time depends on (for
example, $n$ might be the length of a list input to the function)

2. Compute the running time in terms of constants and $n$.

3. Describe the running time using big-Oh notation.

I may very well ask you to do this sort of thing on an exam.
