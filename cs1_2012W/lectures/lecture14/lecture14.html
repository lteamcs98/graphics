<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">
/*<![CDATA[*/
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode, table.sourceCode pre 
   { margin: 0; padding: 0; border: 0; vertical-align: baseline; border: none; }
td.lineNumbers { border-right: 1px solid #AAAAAA; text-align: right; color: #AAAAAA; padding-right: 5px; padding-left: 5px; }
td.sourceCode { padding-left: 5px; }
code.sourceCode span.kw { color: #007020; font-weight: bold; } 
code.sourceCode span.dt { color: #902000; }
code.sourceCode span.dv { color: #40a070; }
code.sourceCode span.bn { color: #40a070; }
code.sourceCode span.fl { color: #40a070; }
code.sourceCode span.ch { color: #4070a0; }
code.sourceCode span.st { color: #4070a0; }
code.sourceCode span.co { color: #60a0b0; font-style: italic; }
code.sourceCode span.ot { color: #007020; }
code.sourceCode span.al { color: red; font-weight: bold; }
code.sourceCode span.fu { color: #06287e; }
code.sourceCode span.re { }
code.sourceCode span.er { color: red; font-weight: bold; }
/*]]>*/
  </style>
</head>
<body>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<link rel="stylesheet" href="http://www.cs.dartmouth.edu/~cs1/azul.css" type="text/css" />


<div id = "menubar">
<ul>
<li><a href="http://www.cs.dartmouth.edu/~cs1/syllabus.html">Syllabus</a>
<li><a href="http://www.cs.dartmouth.edu/~cs1/schedule.html">Schedule</a>
<li><a href="http://www.cs.dartmouth.edu/~cs1/shortassign/short_assignments.html">Short assignments</a>
<li><a href="http://www.cs.dartmouth.edu/~cs1/labs/lab_assignments.html">Labs</a>
<li><a href="http://www.cs.dartmouth.edu/~cs1/exams.html">Exams</a>
<li><a href="http://www.cs.dartmouth.edu/~cs1/software.html">Course software</a>
<li> <a href="http://greenteapress.com/thinkpython/thinkpython.html">Book</a>
<li><A HREF="&#109;&#97;&#105;&#108;&#116;&#111;&#58;%63%73%31%68%65%6C%70%40%63%73%2E%64%61%72%74%6D%6F%75%74%68%2E%65%64%75">Get help</A>
</ul>
</div>

<div id = "termtitle"> CS 1:  Winter 2012 </div> 
<h1 id="short-assignment-9">Short Assignment 9</h1>
<p><a href="../../shortassign/insertion/sa_insertion.html">Short Assignment 9</a> is posted and due on Monday.</p>
<h1 id="analyzing-algorithms-continued">Analyzing algorithms, continued</h1>
<p>Last time, we started to look at how to analyze algorithms. One of the issues we discussed was base-2 logarithms.</p>
<h2 id="analyzing-the-running-time-for-a-program">Analyzing the running time for a program</h2>
<p>So far, we've only talked loosely about the running time of algorithms. We can see that binary search, given the worst possible input for a binary search, will require fewer &quot;steps&quot; to complete than a linear search, given the worst possible input for linear search. Let's try to do a more precise analysis, given some code that implements an algorithm.</p>
<p>Here is my implementation of linear search, from <a href="linear_search.py">linear_search.py</a>. (I have removed comments to save space. Of course, your own implementation would have comments, rrrright?)</p>
<pre class="sourceCode"><code class="sourceCode python"><span class="kw">def</span> linear_search(the_list, key):<br />    index = <span class="dv">0</span><br />    <span class="kw">while</span> index &lt; <span class="dt">len</span>(the_list):<br />        <span class="kw">if</span> key == the_list[index]:<br />            <span class="kw">return</span> index<br />        <span class="kw">else</span>:<br />            index += <span class="dv">1</span><br /><br />    <span class="kw">return</span> <span class="ot">None</span></code></pre>
<p>Let's say that <code>the_list</code> has the length <span class="math"><em>n</em></span>. Let's analyze how much time the function will take to run in terms of <span class="math"><em>n</em></span>.</p>
<p>First, Python has to create a local variable <code>index</code>. That's pretty fast, and it's done only once, but let's measure that time anyway, and call it <span class="math"><em>c</em><sub>1</sub></span>, where <span class="math"><em>c</em><sub>1</sub></span> is some constant that does not depend in any way on the length <span class="math"><em>n</em></span> of <code>the_list</code>.</p>
<p>Now let's look at the while-loop. In a worst-case analysis, we have to assume that the item is not in the list at all. The body of the while-loop will execute <span class="math"><em>n</em></span> times. It seems reasonable to assume that each execution of the body will take a constant amount of time (or at least an amount upper-bounded by a constant); let's call it <span class="math"><em>c</em><sub>2</sub></span>. So the total time spent executing the body of the while-loop, in the worst case, is <span class="math"><em>c</em><sub>2</sub><em>n</em></span>.</p>
<p>But we're not quite done. In the worst case, the last test in the while-loop header comes up <code>False</code>, and it's when <code>index</code> equals <code>len(the_list)</code>. That's the <span class="math">(<em>n</em> + 1)</span>st time we make that test (because we've already made the tests for <code>index</code> equaling <span class="math">0, 1, 2, …, <em>n</em> - 1</span>). We also have to return <code>None</code> in the worst case. That last test and returning <code>None</code>, together, take some constant amount of time; let's call it <span class="math"><em>c</em><sub>3</sub></span> (again, independent of <span class="math"><em>n</em></span>).</p>
<p>Now, as they say in a French restaurant, &quot;L'addition s'il vous plait.&quot; (&quot;The check, please.&quot;) The running time of linear search in the worst case is therefore no worse than</p>
<p><span class="math"><em>c</em><sub>1</sub> + <em>c</em><sub>2</sub><em>n</em> + <em>c</em><sub>3</sub></span> ,</p>
<p>or</p>
<p><span class="math"><em>c</em><sub>2</sub><em>n</em> + (<em>c</em><sub>1</sub> + <em>c</em><sub>3</sub>)</span> .</p>
<p>The exact values of <span class="math"><em>c</em><sub>1</sub></span>, <span class="math"><em>c</em><sub>2</sub></span>, and <span class="math"><em>c</em><sub>3</sub></span> depend on the speed of the computer the program is run on and the efficiency of the Python implementation.</p>
<p>Here is my implementation of binary search from Short Assignment 8, in <a href="../../shortassign/binarysearch/solution/binary_search.py">binary_search.py</a>:</p>
<pre class="sourceCode"><code class="sourceCode python"><span class="co"># Perform binary search for key on the sublist of the_list</span><br /><span class="co"># starting at index left and going up to and including index right.</span><br /><span class="co"># If key appears in the_list, return the index where it appears.</span><br /><span class="co"># Otherwise, return None.</span><br /><span class="co"># Requires the_list to be sorted.</span><br /><span class="kw">def</span> binary_search(the_list, key, left = <span class="ot">None</span>, right = <span class="ot">None</span>):<br />    <span class="co"># If using the default parameters, then search the entire list.</span><br />    <span class="kw">if</span> left == <span class="ot">None</span> and right == <span class="ot">None</span>:<br />        left = <span class="dv">0</span><br />        right = <span class="dt">len</span>(the_list) - <span class="dv">1</span><br /><br />    <span class="co"># If the list is empty, then the key is not found.</span><br />    <span class="kw">if</span> left &gt; right:<br />        <span class="kw">return</span> <span class="ot">None</span><br />    <span class="kw">else</span>:<br />        <span class="co"># Find the midpoint of this sublist.</span><br />        midpoint = (left + right) / <span class="dv">2</span><br /><br />        <span class="co"># Is it there?</span><br />        <span class="kw">if</span> the_list[midpoint] == key:<br />            <span class="kw">return</span> midpoint     <span class="co"># yes!</span><br />        <span class="kw">elif</span> the_list[midpoint] &gt; key:<br />            <span class="co"># Try the left half of the_list.</span><br />            <span class="kw">return</span> binary_search(the_list, key, left, midpoint<span class="dv">-1</span>)<br />        <span class="kw">else</span>:<br />            <span class="co"># Try the right half of the list.</span><br />            <span class="kw">return</span> binary_search(the_list, key, midpoint<span class="dv">+1</span>, right)</code></pre>
<p>If we look at the time for each recursive call on its own, not counting the time for the recursive calls that it makes, we see that each recursive call takes a constant amount of time. We check whether we have an empty sublist, compute the midpoint, check whether the key is in the position given by the midpoint, and recursively call the function on the proper half. Each of these steps, <em>within a given recursive call</em> takes constant time; let's call it <span class="math"><em>c</em><sub>4</sub></span>. And, as we've seen, in the worst case, the number of recursive calls is at most <span class="math">log<sub>2</sub><em>n</em> + 1</span>. So the total worst-case time for binary search is</p>
<p><span class="math"><em>c</em><sub>4</sub>(log<sub>2</sub><em>n</em> + 1)</span> ,</p>
<p>or</p>
<p><span class="math"><em>c</em><sub>4</sub>log<sub>2</sub><em>n</em> + <em>c</em><sub>4</sub></span> .</p>
<h2 id="comparing-linear-search-and-binary-search">Comparing linear search and binary search</h2>
<p>So which is faster, linear search or binary search? It depends: on the suitability of the input we get for the particular algorithm (linear search will find &quot;Afganistan&quot; faster than binary search, for the example list), on the length of the list, and on the size of the constants. In the worst case, the question is, when is</p>
<p><span class="math"><em>c</em><sub>4</sub>log<sub>2</sub><em>n</em> + <em>c</em><sub>4</sub></span></p>
<p>smaller than</p>
<p><span class="math"><em>c</em><sub>2</sub><em>n</em> + (<em>c</em><sub>1</sub> + <em>c</em><sub>3</sub>)</span> ?</p>
<p>I claim that whatever the value of the constants, for large enough <span class="math"><em>n</em></span>, binary search will be faster, if you assume the worst-case input for each algorithm. Let's say that linear search is run on a HAL 9000 computer that is blazingly fast and ultra-modern, and that binary search is run on a vintage Apple II. Let's assume that the HAL 9000 executes each iteration of the while-loop in linear search in 0.00001 seconds, so that <span class="math"><em>c</em><sub>2</sub> = 0.00001</span>, and let's say that <span class="math"><em>c</em><sub>1</sub> + <em>c</em><sub>3</sub> = 0</span>.</p>
<p>Now let's take the Apple II. It is much slower, and it takes 0.01 seconds to execute each recursive call of binary search, so that <span class="math"><em>c</em><sub>4</sub> = 0.01</span>.</p>
<p>How large does <span class="math"><em>n</em></span> have to be before the Apple II wins? I claim that <span class="math">log<sub>2</sub>16384 = 14</span>; you can check for yourself that <span class="math">2<sup>14</sup> = 16384</span>. So, when <span class="math"><em>n</em> = 16384</span>, the Apple II takes <span class="math">0.01 × 14 + 0.01 = 0.14 + 0.01 = 0.15</span> seconds. For the same input size, the HAL 9000 takes <span class="math">0.00001 × 16384 = 0.16384</span> seconds. The Apple II wins!</p>
<p>Perhaps you're thinking that I rigged the game here by choosing constants <span class="math"><em>c</em><sub>2</sub></span> and <span class="math"><em>c</em><sub>4</sub></span> so that the Apple II would win. (The HAL 9000 would not permit me to do that.) Let's just choose a longer list. Consider a list of length 1 million. Since <span class="math">log<sub>2</sub>10<sup>6</sup></span> is about 20, the Apple II with binary search will take about <span class="math">0.2</span> seconds for the search. The HAL 9000 will take <span class="math">10</span> seconds. There's no way that the difference between <span class="math"><em>c</em><sub>2</sub></span> and <span class="math"><em>c</em><sub>4</sub></span> will overcome the Apple II's <span class="math">9. 8</span>-second lead. From now on, we will ignore these leading constant coefficients when comparing algorithms, since once <span class="math"><em>n</em></span> gets large enough, the constants become less important than how the running time varies with <span class="math"><em>n</em></span>.</p>
<h2 id="orders-of-growth-and-big-oh-notation">Orders of growth and big-Oh notation</h2>
<p>We say that <strong>in the worst case, linear search takes linear time in <span class="math"><em>n</em></span>, the size of the input list</strong> because the running time scales linearly with <span class="math"><em>n</em></span>. If the length of the input list doubles, the running time of linear search doubles, in the worst case. If the size of the input list quadruples, the running time quadruples, in the worst case.</p>
<p>We say that <strong>in the worst case, binary search takes logarithmic time in <span class="math"><em>n</em></span>, the size of the input list</strong> because the running time scales like a <span class="math">log<sub>2</sub></span> function. If the original size of the list doubles from <span class="math"><em>n</em></span> to <span class="math">2<em>n</em></span>, the running time increases from <span class="math"><em>c</em><sub>4</sub> log<sub>2</sub><em>n</em> + <em>c</em><sub>4</sub></span> to <span class="math"><em>c</em><sub>4</sub> log<sub>2</sub>(2<em>n</em>) + <em>c</em><sub>4</sub></span>. How much bigger is this running time? Forget about the additive <span class="math"><em>c</em><sub>4</sub></span> term; it's insignificant. We can analyze <span class="math"><em>c</em><sub>4</sub> log<sub>2</sub><em>n</em></span> vs. <span class="math"><em>c</em><sub>4</sub> log<sub>2</sub>(2<em>n</em>)</span> either by thinking about the algorithm or with some equations. Thinking about the algorithm, doubling the length of the list will require that binary search recurse just one more time. That's not very expensive; it just costs <span class="math"><em>c</em><sub>4</sub></span> more time, even though we <em>doubled</em> the size of the list. If you prefer to use equations, <span class="math"><em>c</em><sub>4</sub> log<sub>2</sub>(2<em>n</em>) = <em>c</em><sub>4</sub> log<sub>2</sub><em>n</em> + <em>c</em><sub>4</sub> log<sub>2</sub>2 = <em>c</em><sub>4</sub> log<sub>2</sub><em>n</em> + <em>c</em><sub>4</sub></span>.</p>
<p>We can see that in general, algorithms that take logarithmic time are always faster than algorithms that take linear time, at least once <span class="math"><em>n</em></span> is large enough. As <span class="math"><em>n</em></span> gets even bigger, the algorithm with logarithmic time will win by even more. The function <span class="math">log<sub>2</sub><em>n</em></span> grows much more slowly than the function <span class="math"><em>n</em></span>.</p>
<p>We are usually only concerned with running time of an algorithm once its input size <span class="math"><em>n</em></span> gets large. For large enough <span class="math"><em>n</em></span>, a logarithmic running time is better than a linear running time, <em>regardless of the constants</em>. Computer scientists use a standard notation to indicate that the running time grows &quot;like&quot; some function, ignoring the constants. If the running time is linear (grows linearly with <span class="math"><em>n</em></span>) in the worst case, we say that the running time is <span class="math"><em>O</em>(<em>n</em>)</span>, pronounced &quot;big-Oh of <span class="math"><em>n</em></span>&quot;. If the running time is logarithmic (grows with the logarithm of <span class="math"><em>n</em></span>), we say that the running time is <span class="math"><em>O</em>(log<sub>2</sub><em>n</em>)</span>.</p>
<p>Big-oh notation indicates that for large enough <span class="math"><em>n</em></span>, ignoring constants, the running time is no worse than the function in the parentheses. I can therefore say that binary search's running time is <span class="math"><em>O</em>(log<sub>2</sub><em>n</em>)</span> and that linear search's running time is <span class="math"><em>O</em>(<em>n</em>)</span>.</p>
<p>In fact, we can drop the base of the logarithm when we use big-Oh notation. Why? Because of this mathematical fact: for any constants <span class="math"><em>a</em></span> and <span class="math"><em>b</em></span>,</p>
<p><span class="math">$\displaystyle \log_a n = \frac{\log_b n}{\log_b a}$</span> .</p>
<p>So the difference between taking the logarithm of <span class="math"><em>n</em></span> using base <span class="math"><em>a</em></span> vs. base <span class="math"><em>b</em></span> is just the constant factor <span class="math">log<sub><em>b</em></sub><em>a</em></span>, and we've already decided that constant factors don't matter in big-Oh notation.</p>
<p>I slipped something in before, when I said that &quot;Big-oh notation indicates that for large enough <span class="math"><em>n</em></span>, ignoring constants, the running time <em>is no worse than</em> the function in the parentheses.&quot; Big-Oh notation gives us an <strong>upper bound</strong> on the running time's rate of growth, but it doesn't say <em>anything</em> about how low the running time's rate of growth might be. If I tell you that I have an amount of money in my wallet, and I guarantee that this amount is at most a thousand dollars, I might have a thousand dollars, or I might have only ten cents.</p>
<p>So if an algorithm's running time is <span class="math"><em>O</em>(<em>n</em>)</span>, it might actually take <span class="math"><em>c</em><em>n</em></span> time for some constant <span class="math"><em>c</em></span>, but it might be even faster. For example, it might take only <span class="math"><em>c</em> log<sub>2</sub><em>n</em></span> time. In other words, any running time that is <span class="math"><em>O</em>(log <em>n</em>)</span> is also <span class="math"><em>O</em>(<em>n</em>)</span>: it's at least as good as linear, if not better. Put another way: it is true that <em>binary</em> search runs in <span class="math"><em>O</em>(<em>n</em>)</span> time, but that is not the most accurate statement you can make; it would be more accurate to say that binary search runs in <span class="math"><em>O</em>(log <em>n</em>)</span> time.</p>
<h2 id="other-orders-of-growth-constant-and-quadratic-orders-of-growth">Other orders of growth: Constant and quadratic orders of growth</h2>
<p>Are there algorithms that are <span class="math"><em>O</em>(<em>n</em><sup>2</sup>)</span>? Sure. Here's one:</p>
<pre class="sourceCode"><code class="sourceCode python"><span class="kw">def</span> do_something(n):<br />    <span class="kw">for</span> i in <span class="dt">range</span>(n):<br />        <span class="kw">for</span> j in <span class="dt">range</span>(n):<br />            <span class="kw">print</span> i, j        </code></pre>
<p>Let's assume that the print statement takes some constant amount of time to execute, <span class="math"><em>c</em></span> seconds. How many times will that statement (the body of the innermost loop) be executed? It looks like <span class="math"><em>n</em><sup>2</sup></span> times. So the runtime is roughly <span class="math"><em>c</em><em>n</em><sup>2</sup></span>. Ignoring the constants, we say that the function has a running time of <span class="math"><em>O</em>(<em>n</em><sup>2</sup>)</span>.</p>
<p>What if an algorithm takes constant time, regardless of the input?</p>
<pre class="sourceCode"><code class="sourceCode python"><span class="kw">def</span> do_something_else(n):<br />    <span class="kw">print</span> <span class="st">&quot;My name is Inigo Montoya&quot;</span></code></pre>
<p>We say in this case that the function is <span class="math"><em>O</em>(1)</span>, since <span class="math">1</span> is what we get for the running time if we drop the constants.</p>
<h2 id="ranking-algorithms-by-running-time">Ranking algorithms by running time</h2>
<p>It's true that not all <span class="math"><em>O</em>(<em>n</em><sup>2</sup>)</span> algorithms take the same time. The constants might be different, and the best-case performance may be different for different algorithms. However, it is true that in the worst case, for large enough <span class="math"><em>n</em></span>, any <span class="math"><em>O</em>(<em>n</em><sup>2</sup>)</span>-time algorithm runs faster than any algorithm for which the runtime is any positive constant times <span class="math"><em>n</em><sup>3</sup></span>. We therefore can think of all <span class="math"><em>O</em>(<em>n</em><sup>2</sup>)</span> algorithms as being roughly in the same family. We can do a ranking by running time:</p>
<p><span class="math">$O(1) &lt; O(\log\: n) &lt; O(\sqrt n) &lt; O(n) &lt; O(n\: \log\: n) &lt; O(n^2) &lt; O(n^3) &lt; O(2^n) &lt; O(n!)$</span></p>
<p>An algorithm whose rate of growth depends on the factorial of the input <span class="math"><em>n</em></span> is going to run for a very long time indeed, for large values of <span class="math"><em>n</em></span>.</p>
<p>Sometimes, you will see a running time function that looks like this:</p>
<p><span class="math"><em>c</em><sub>1</sub><em>n</em><sup>2</sup> + <em>c</em><sub>2</sub><em>n</em> + <em>c</em><sub>3</sub></span>.</p>
<p>In this case, we can drop all of the lower-order terms and say that the function is <span class="math"><em>O</em>(<em>n</em><sup>2</sup>)</span>. Why? For large enough <span class="math"><em>n</em></span>, <span class="math"><em>n</em><sup>2</sup></span> grows much more quickly than <span class="math"><em>n</em></span>. For large <span class="math"><em>n</em></span>, another function <span class="math"><em>c</em><sub>4</sub><em>n</em><sup>2</sup></span> would be larger than <span class="math"><em>c</em><sub>1</sub><em>n</em><sup>2</sup> + <em>c</em><sub>2</sub><em>n</em> + <em>c</em><sub>3</sub></span>, if <span class="math"><em>c</em><sub>4</sub></span> is even a little bit larger than <span class="math"><em>c</em><sub>1</sub></span>. Since <span class="math"><em>c</em><sub>4</sub><em>n</em><sup>2</sup></span> is <span class="math"><em>O</em>(<em>n</em><sup>2</sup>)</span>, <span class="math"><em>c</em><sub>1</sub><em>n</em><sup>2</sup> + <em>c</em><sub>2</sub><em>n</em> + <em>c</em><sub>3</sub></span> must be, too.</p>
<p>The take home message: compute the running time for a function, using constants if you need to. Then drop the constants and just take the term in the expression that grows the fastest. The running time is big-Oh of that term.</p>
<h2 id="what-is-this-n-anyway">What is this &quot;<span class="math"><em>n</em></span>&quot; anyway?</h2>
<p>The value <span class="math"><em>n</em></span> represents some characteristic of the input that determines the running time. Maybe <span class="math"><em>n</em></span> is the length of a list that's input to the function, or maybe <span class="math"><em>n</em></span> is the value of some parameter that is input to the function. We use <span class="math"><em>n</em></span> instead of saying specifically what the term is so that we can discuss the rate of growth of the running time of an algorithm in more general terms. However, we should always know specifically to what <span class="math"><em>n</em></span> refers before discussing the running time.</p>
<h2 id="exercises-and-exams">Exercises and exams</h2>
<p>From now on, given an algorithm or piece of code, you should be able to</p>
<ol style="list-style-type: decimal">
<li><p>Identify what quantity <span class="math"><em>n</em></span> the running time depends on (for example, <span class="math"><em>n</em></span> might be the length of a list input to the function)</p></li>
<li><p>Compute the running time in terms of constants and <span class="math"><em>n</em></span>.</p></li>
<li><p>Describe the running time using big-Oh notation.</p></li>
</ol>
<p>I may very well ask you to do this sort of thing on an exam.</p>
<h1 id="merge-sort">Merge sort</h1>
<p>Next, we're going to use recursion to sort a list. We've already seen the selection sort algorithm, and in Short Assignment 9, you'll implement the insertion sort algorithm which, like selection sort, is not recursive. In Lab Assignment 3, you'll implement the quicksort algorithm, which is recursive.</p>
<p>The recursive algorithm we'll examine now, <strong>merge sort</strong>, runs in <span class="math"><em>O</em>(<em>n</em> log <em>n</em>)</span> time in all cases. We haven't analyzed selection sort or insertion sort yet, but they take <span class="math"><em>O</em>(<em>n</em><sup>2</sup>)</span> time in the worst case. In the worst case, therefore, merge sort is better than either selection sort or insertion sort: we trade a factor of <span class="math"><em>n</em></span> for a factor of only <span class="math">log <em>n</em></span>, and you take that trade any day.</p>
<p>The constants hidden in the big-Oh notation are not as good for merge sort as they are for the other two sorting algorithms, however, and so merge sort is not the sorting algorithm of choice for small problem sizes. Once the problem size is in the range 500–1000, merge sort beats the other two, and its advantage increases as the problem size increases from there.</p>
<p>Another potential disadvantage of merge sort is that it does not work in place. That is, it has to make complete copies of the entire input list. Contrast this feature with selection sort and, as you'll see in Short Assignment 9, insertion sort, which at any time keep a copy of only one list entry rather than copies of all the list entries. Thus, if space is at a premium, merge sort might not be the sorting algorithm of choice.</p>
<p>For most occasions, however, merge sort will be just fine.</p>
<h3 id="linear-time-merging">Linear-time merging</h3>
<p>The key to making merge sort work is a linear-time merging step. The idea is follows. Suppose we have two sorted lists, say <code>a[0:m]</code>, containing <span class="math"><em>m</em></span> items, and <code>b[0:n]</code>, containing <span class="math"><em>n</em></span> items, and we wish to produce a sorted list <code>c[0:m+n]</code> containing all <span class="math"><em>m</em> + <em>n</em></span> items in either <code>a</code> or <code>b</code>.</p>
<p>Here is an important observation: there are only two candidates for the value that should be in <code>c[0]</code> (the smallest item of the output), and these candidates are <code>a[0]</code> and <code>b[0]</code>. Why? Because the lists <code>a</code> and <code>b</code> are sorted, the smallest item overall must be the smallest item of whichever list, <code>a</code> or <code>b</code>, it started in. So we take the smaller of <code>a[0]</code> and <code>b[0]</code> and copy it into <code>c[0]</code>.</p>
<p>Let's say that it turned out that <code>a[0] &lt; b[0]</code>, so that we copied <code>a[0]</code> into <code>c[0]</code>. The value that should go into <code>c[1]</code> is the second-smallest item overall, and it is also the smallest item of those remaining in <code>a[1:m]</code> and <code>b[0:n]</code>. Using the same reasoning as before, there are only two candidates for this value: <code>a[1]</code> and <code>b[0]</code>. We pick the smaller of these values and copy it into <code>c[1]</code>.</p>
<p>In general, as we are copying values into <code>c</code>, we have to look at only two candidate values: the smallest remaining value in <code>a</code> and the smallest remaining value in <code>b</code>. It takes a constant amount of time to determine which of these values is smaller, to copy it into the correct position of <code>c</code>, and to update any indices into lists <code>a</code>, <code>b</code>, and <code>c</code>. In other words, merging costs us only <span class="math"><em>O</em>(1)</span> time per item merged. The time to copy all <code>m+n</code> input values to the output list <code>c</code>, therefore, is (m+n) times $O(1)~, which is <span class="math"><em>O</em>(<em>m</em> + <em>n</em>)</span>, or linear in the size of the input.</p>
<p>To see linear-time merging in action, look at the slide show in <a href="Linear-time-merge.ppt">this PowerPoint</a>.</p>
<p>To see linear-time merging in code, look at the <code>merge</code> function in <a href="merge_sort.py">merge_sort.py</a>:</p>
<pre class="sourceCode"><code class="sourceCode python"><span class="co"># merge_sort.py</span><br /><span class="co"># Merge sort example for cs1</span><br /><span class="co"># Devin Balkcom</span><br /><span class="co"># October, 2011</span><br /><span class="co"># Modified by THC.</span><br /><br /><span class="co"># Take two sorted lists, the_list[p : q+1] and the_list[q+1 : r+1],</span><br /><span class="co"># and merge them into the_list[p : r+1].</span><br /><span class="kw">def</span> merge(the_list, p, q, r):<br />    <span class="co"># Make a copy of the list items.</span><br />    left = the_list[p : q<span class="dv">+1</span>]<br />    right = the_list[q<span class="dv">+1</span> : r<span class="dv">+1</span>]<br /><br />    <span class="co"># Until we've gone through one of left and right, move</span><br />    <span class="co"># the smallest unmerged item into the next position in</span><br />    <span class="co"># the_list[p : r+1].</span><br /><br />    i = <span class="dv">0</span>       <span class="co"># index into left</span><br />    j = <span class="dv">0</span>       <span class="co"># index into right</span><br />    k = p       <span class="co"># index into the_list[p : r+1]</span><br /><br />    <span class="kw">while</span> i &lt; <span class="dt">len</span>(left) and j &lt; <span class="dt">len</span>(right):<br />        <span class="kw">if</span> left[i] &lt; right[j]:<br />            the_list[k] = left[i]<br />            i += <span class="dv">1</span><br />        <span class="kw">else</span>:<br />            the_list[k] = right[j]<br />            j += <span class="dv">1</span><br />        k += <span class="dv">1</span><br /><br />    <span class="co"># We've gone through one of left and right entirely.</span><br />    <span class="co"># Copy the remainder of the other to the end of the_list[p : r+1].</span><br /><br />    <span class="co"># If left has remaining items, copy them into the_list, using list slices.</span><br />    <span class="kw">if</span> i &lt; <span class="dt">len</span>(left):<br />        the_list[k : r<span class="dv">+1</span>] = left[i:]<br /><br />    <span class="co"># If right has remaining items, copy them into the_list, using list slices.</span><br />    <span class="kw">if</span> j &lt; <span class="dt">len</span>(right):<br />        the_list[k : r<span class="dv">+1</span>] = right[j:]<br /><br /><span class="co"># Sort the_list[p : r+1], using merge sort.</span><br /><span class="kw">def</span> merge_sort(the_list, p = <span class="ot">None</span>, r = <span class="ot">None</span>):<br />    <span class="co"># If using the default parameters, sort the entire list.</span><br />    <span class="kw">if</span> p == <span class="ot">None</span> and r == <span class="ot">None</span>:<br />        p = <span class="dv">0</span><br />        r = <span class="dt">len</span>(the_list) - <span class="dv">1</span><br /><br />    <span class="kw">if</span> p &lt; r:   <span class="co"># nothing to do if the sublist has fewer than 2 items</span><br />        q = (p + r) / <span class="dv">2</span>                 <span class="co"># midpoint of p and r</span><br />        merge_sort(the_list, p, q)      <span class="co"># recursively sort the_list[p : q+1]</span><br />        merge_sort(the_list, q<span class="dv">+1</span>, r)    <span class="co"># recursively sort the_list[q+1 : r+1]</span><br />        merge(the_list, p, q, r)        <span class="co"># and merge them together</span><br /><br />l = [<span class="dv">19</span>, <span class="dv">18</span>, <span class="dv">24</span>, <span class="dv">72</span>, <span class="dv">16</span>, <span class="dv">49</span>, <span class="dv">13</span>, <span class="dv">12</span>, <span class="dv">1</span>, <span class="dv">66</span>]<br />merge_sort(l)<br /><span class="kw">print</span> l</code></pre>
<p>Notice that we can copy part of one list into another using <strong>slice notation</strong>, that is the colon within the square brackets. So this line:</p>
<pre class="sourceCode"><code class="sourceCode python">the_list[k : r<span class="dv">+1</span>] = left[i:]</code></pre>
<p>copies the items in <code>left</code> starting at <code>i</code> into <code>the_list</code>, starting at index <code>k</code>.</p>
<p>The merge function merges the sorted sublists <code>the_list[p : q+1]</code> (containing the items from index <code>p</code> through index <code>q</code>—remember that with slice notation, the index after the colon is 1 greater than the index of the last item in the sublist) and and <code>the_list[q+1 : r+1]</code> (containing the items from index <code>q+1</code> through index <code>r</code>) into the sorted sublist <code>the_list[p : r+1]</code>. Notice that here the lists being merged are actually two consecutive sublists of <code>the_list</code>, and that they are being merged into the same locations, indices <code>p</code> through <code>r</code>, that they started in. The <code>merge</code> function creates two new temporary lists, <code>left</code> and <code>right</code>, copied from the original list. It then merges back into the original list. It repeatedly chooses the smaller of the unmerged items in <code>left</code> and <code>right</code> and copies it back into <code>the_list</code>. The index <code>i</code> always tells us the where to find the next unmerged item in <code>left</code>; <code>j</code> indexes the next unmerged item in <code>right</code>, and <code>k</code> indexes where the next merged item will go in <code>the_list</code>. Eventually, we exhaust either <code>left</code> or <code>right</code>. At that time, the other list still has unmerged items, and we can just copy them back into <code>the_list</code> directly.</p>
<h3 id="divide-and-conquer">Divide-and-Conquer</h3>
<p>Now that we have the linear-time merging function <code>merge</code>, we need to use it well. Merge sort works by a common computer-science paradigm known as <strong>divide-and-conquer</strong>:</p>
<ol style="list-style-type: decimal">
<li><strong>Divide</strong> the problem into (approximately) equal-size subproblems.</li>
<li><strong>Conquer</strong> by solving the subproblems <em>recursively</em>.</li>
<li><strong>Combine</strong> the solutions to the subproblems to solve the original problem.</li>
</ol>
<p>For merge sort, the problem to be solved is sorting the sublist <code>the_list[p : r+1]</code>. Initially, for a list with <span class="math"><em>n</em></span> items, we'll have <code>p</code> = 0 and <code>r</code> = <span class="math"><em>n</em>–1</span>, but these values will be different for the recursive calls. Merge sort follows the divide-and-conquer paradigm as follows:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Divide</strong> by finding the index <code>q</code> of the midpoint of the sublist <code>the_list[p : r+1]</code>.</p></li>
<li><p><strong>Conquer</strong> by recursing on the sublists <code>the_list[p : q+1]</code> and <code>the_list[q+1 : r+1]</code>. This step sorts the sublist <code>the_list[p : q+1]</code>, and it sorts the sublist <code>the_list[q+1 : r+1]</code>.</p></li>
<li><p><strong>Combine</strong> by merging the sorted sublist <code>the_list[p : q+1]</code> and <code>the_list[q+1 : r+1]</code> to produce the sorted sublist <code>thelist[p : r+1]</code>. We merge by calling the <code>merge</code> function.</p></li>
</ol>
<p>The recursion &quot;bottoms out&quot; when the sublist to be sorted has fewer than 2 items, since a sublist with 0 or 1 items is already sorted.</p>
<p>The <code>merge</code>_sort function shows how easy divide-and-conquer is for merge sort. It almost seems like cheating! But if you understand recursion, you'll believe that it works.</p>
<p>If you don't understand how the recursion works—I mean <em>really</em> understand it—I recommend that you</p>
<ul>
<li><p>use the debugging tool to step through the code,</p></li>
<li><p>draw the stack frames in the call stack as you step through the code by hand,</p></li>
<li><p>draw out the recursion tree for an example list, labeling each node by the values of the parameters <code>p</code> and <code>r</code> to <code>merge_sort</code>.</p></li>
</ul>
<h2 id="sorting-a-list-of-objects">Sorting a list of objects</h2>
<p>The merge sort function we saw is fine for sorting lists of items that can be easily compared, like strings or ints. What if we wanted to sort a list that contained addresses of (references to) objects? How does Python know which object is &quot;less than&quot; another?</p>
<p>If you are using the Python <code>.sort</code> method, it turns out that you can write a special method <code>__lt__</code> in the class definition. <code>__lt__</code> is then used to compare the objects when you use the less-than symbol &lt;.</p>
<p>But here, we're not using the Python <code>.sort</code> method. We're writing our own sorting code. How can we specify how we want to compare objects?</p>
<p>Imagine that you have a list of planets. You might wish to sort by mass, by distance from the sun, or by mean orbital velocity. These quantities might be stored in instance variables of a <code>Planet</code> object, or they might be computable using instance variables of a <code>Planet</code> object.</p>
<p>There is only one place in the merge sort code where we compare items from the list:</p>
<pre class="sourceCode"><code class="sourceCode python">        <span class="kw">if</span> left[i] &lt; right[l]:</code></pre>
<p>We need a function to replace the <code>&lt;</code>. Let's say we had two <code>Planet</code> objects, each with a <code>mass</code> instance variable. We could write the function like this:</p>
<pre class="sourceCode"><code class="sourceCode python"><span class="kw">def</span> lessthan_mass(body1, body2):<br />    <span class="kw">return</span> body1.mass &lt; body2.mass</code></pre>
<p>I included &quot;mass&quot; in the function name to make it clear that this &quot;less than&quot; function compares masses. Somehow I need to specify to the merge sort code that <code>merge</code> should use the <code>lessthan_mass</code> function instead of a simple <code>&lt;</code> sign in comparisons. Fortunately, we can pass a function as a parameter to another function. We rewrite the header for <code>merge_sort</code> like this:</p>
<pre class="sourceCode"><code class="sourceCode python"><span class="kw">def</span> merge_sort(the_list, p, r, compare_fn)</code></pre>
<p>and for <code>merge</code>:</p>
<pre class="sourceCode"><code class="sourceCode python"><span class="kw">def</span> merge(the_list, p, q, r, compare_fn):</code></pre>
<p>When <code>merge_sort</code> is called, we just pass in <code>lessthan_mass</code> (itself a function) as the second parameter. Within <code>merge_sort</code>, we change the call to <code>merge</code> to include <code>compare_fn</code> as the second actual parameter. Finally, in the body of <code>merge</code>, we can use <code>compare_fn</code> to compare items rather than using the <code>&lt;</code> sign:</p>
<pre class="sourceCode"><code class="sourceCode python">        <span class="kw">if</span> compare_fn(left[i], right[j]):</code></pre>
<h2 id="the-recursion-tree-for-merge-sort">The recursion tree for merge sort</h2>
<p>I find drawing the recursion tree to be one of the most helpful ways to understand or debug a recursive function. The recursion tree can be used to easily show:</p>
<ol style="list-style-type: decimal">
<li>What functions each function calls, and how the problem is broken into smaller problems.</li>
<li>The order functions are called in.</li>
<li>The base cases of the recursion, at the leaves.</li>
</ol>
<p>Here is the recursion tree for merge sort:</p>
<div class="figure">
<img src="merge_tree.png" /><p class="caption"></p>
</div>
<p>The calls to <code>merge</code> are in red. The order of the function calls is shown by small numbers above and to the left of each function call. We can also see the parameters of the functions. To save space, I abbreviated calls to <code>merge_sort</code> by <code>msort</code> and ommited the name of the list as a parameter.</p>
<p>You should be able to reconstruct the same tree by just reading and analyzing the code for merge sort, and you should be able to draw the recursion tree for any new recursive code you see or write.</p>
<h2 id="analyzing-runtime-for-recursive-function-calls">Analyzing runtime for recursive function calls</h2>
<p>The recursion tree is also often a very good way to analyze the running time of a recursive algorithm. In the following picture, I've used dashed lines to separate the recursion tree for a sample call to <code>merge_sort</code> into four layers.</p>
<div class="figure">
<img src="merge_runtime.png" /><p class="caption"></p>
</div>
<p>The top layer contains the initial call to <code>merge_sort</code> and its call to <code>merge</code>. The second layer contains the two merge sorts of half-size lists. The third layer contains the four merge sorts of quarter-size lists. The fourth layer contains the eight merge sorts of eighth-size lists.</p>
<p>Let's analyze the running time of each layer. A call to <code>merge_sort</code> has two types of costs:</p>
<ol style="list-style-type: decimal">
<li>The cost associated with all lines of code except function calls to <code>merge</code> or <code>merge_sort</code>. Let's call this a constant, <span class="math"><em>c</em></span>.</li>
<li>The cost associated with calls to <code>merge</code> and <code>merge_sort</code>.</li>
</ol>
<p>When we account for the runing time, we'll charge the second types of cost to the functions where the work is actually done. For example, when we compute the cost of the top layer, we won't include the costs incurred by layer 2.</p>
<ul>
<li><p><strong>Layer 1 (top layer).</strong> There is one call to <code>merge_sort</code>. The charge for that is <span class="math"><em>c</em></span>. There is one call to <code>merge</code>. If we analyze the running time of <code>merge</code>, the cost is some constant, <span class="math"><em>k</em></span>, times the length of the sublist. So <code>merge(0, 7)</code> costs <span class="math">8<em>k</em></span> time, or <span class="math"><em>n</em><em>k</em></span> time. The overall cost of the layer is <span class="math">1 × (<em>c</em> + <em>n</em><em>k</em>)</span>.</p></li>
<li><p><strong>Layer 2.</strong> There are two calls to <code>merge_sort</code>. The charge for that is <span class="math">2<em>c</em></span>. There are two calls to <code>merge</code>. The charges are each <span class="math"><em>n</em> / 2 × <em>k</em></span>, for a total &quot;merging charge&quot; of <span class="math"><em>n</em><em>k</em></span>. The overall cost of the layer is <span class="math">2<em>c</em> + <em>n</em><em>k</em></span>.</p></li>
<li><p><strong>Layer 3.</strong> Four calls to <code>merge_sort</code>: <span class="math">4<em>c</em></span>. Four calls to <code>merge</code>, each of size <span class="math"><em>n</em> / 4</span>, giving a cost of <span class="math"><em>n</em><em>k</em></span>. Total cost: <span class="math">4<em>c</em> + <em>n</em><em>k</em></span>.</p></li>
<li><p><strong>Layer 4.</strong> Eight calls to <code>merge_sort</code>s: <span class="math">8<em>c</em></span>. No merges. Total cost: <span class="math"><em>n</em><em>c</em></span>.</p></li>
</ul>
<p>Overall, we see that, not counting the cost of merging, the <span class="math">2<em>n</em> - 1</span> calls to <code>merge_sort</code> cost <span class="math">(2<em>n</em> - 1)<em>c</em></span> time. Now let's look at the <code>merge</code> costs. Each layer of the tree costs the same for the <code>merges</code>: later layers of the tree had more merges, but the merges were smaller. Layers 1, 2, and 3 cost <span class="math"><em>n</em><em>k</em></span> each. How many layers is the tree? Since we divide the sublist in half for each new layer, there are <span class="math">1 + log<sub>2</sub><em>n</em></span> layers in the tree. The last layer doesn't have a merge, and so the total merge cost is <span class="math"><em>n</em><em>k</em>(log<sub>2</sub><em>n</em>)</span>.</p>
<p>Our total cost for merge sort is therefore <span class="math">(2<em>n</em> - 1)<em>c</em> + (log<sub>2</sub><em>n</em>)<em>n</em><em>k</em></span>. We see that the fastest growing term is <span class="math"><em>n</em><em>k</em> log<sub>2</sub><em>n</em></span>; dropping the constants, we get that the running time of merge sort is <span class="math"><em>O</em>(<em>n</em> log<sub>2</sub><em>n</em>)</span>. When the base of the logarithm is a constant, such as 2, we can drop that, too: <span class="math"><em>O</em>(<em>n</em> log <em>n</em>)</span>.</p>
</body>
</html>
