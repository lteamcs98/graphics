# NP-Completeness

When I buy material products over the Internet, the seller has to get
them delivered to my home.  Most of the time, the seller uses a
package-delivery company.  I won't say which package-delivery company
is most often used for the products I purchase, other than to say that
brown trucks have been known to stop in front of my driveway every now
and then.

## Brown trucks

The package-delivery company operates over 91,000 of these brown
trucks in the U.S., as well as many others worldwide.  Each of these
trucks delivers parcels to several dozen residential and commercial
addresses in each of at least five days per week.  Every day, each
truck starts and ends at a specific depot and drops off parcels at
several dozen locations.  The package-delivery company has a keen
interest in minimizing the cost incurred by each truck as it makes
several dozen stops each day.  For example, one online source I
consulted claimed that once the company mapped out routes for its
drivers to reduce the number of left turns, it reduced the total
distance traveled by its vehicles by 464,000 miles in an 18-month
period, saving over 51,000 gallons of fuel, with the added benefit of
decreasing carbon dioxide emissions by 506 metric tons.

How can the company minimize the cost of sending out each truck each
day?  Suppose that a given truck must deliver parcels to $n$ locations
on a particular day.  Adding in the depot, there are $n+1$ locations
that the truck must visit.  For each of these $n+1$ locations, the
company can calculate the costs of sending the truck to each of the
other $n$ locations, so that the company has an $(n+1) \times (n+1)$
table of costs from location to location, where the entries on the
diagonal are meaningless, since the $i$th row and the $i$th column
correspond to the same location.  The company wants to determine the
route that starts and ends at the depot and visits all other $n$
locations exactly once, such that the total cost of the entire route
is as low as possible.

It is possible to write a computer program that will solve this
problem.  After all, if we consider a particular route and we know the
order of stops on the route, then it's just a matter of looking up in
table the costs of going from location to location and adding them up.
Then we just have to enumerate all the possible routes and keep track
of which one has the lowest total cost.  The number of possible routes
is finite, and so the program will terminate at some point and give
the answer.  That program doesn't seem so hard to write, does it?

Indeed, the program isn't hard to write.

It's hard to run.

The hitch is that the number of possible routes that visit $n$
locations is enormous: $n!$ ($n$-factorial).  Why?  The truck starts
at the depot.  From there, any of the other $n$ locations can be the
first stop.  From the first stop, any of the remaining $n-1$ locations
can be the second stop, and so there are $n \cdot (n-1)$ possible
combinations for the first two stops, in order.  Once we settle on the
first two stops, any of $n-2$ locations could be the third stop,
giving $n \cdot (n-1) \cdot (n-2)$ possible orders for the first three
stops.  Extending this reasoning to the $n$ delivery locations, the
number of possible orders is $n \cdot (n-1) \cdot (n-2) \cdots 3 \cdot
2 \cdot 1$, or $n!$.

Recall that $n!$ grows faster than an exponential function; it's
superexponential.  Suppose that a truck delivers to $20$ addresses per
day.  (In the U.S., the company averages about $170$ packages per
truck, so even allowing for multiple packages to be delivered to a
single location, $20$ stops per day doesn't seem like an
overestimate.)  With $20$ stops, a computer program would have to
enumerate $20!$ possible orders, and $20!$ equals
2,432,902,008,176,640,000.  If the company's computers could enumerate
and evaluate one trillion orders per second, it would require over
$28$ days to try them all.  And that's for just one day's worth of
deliveries for one of over 91,000 trucks.

With this approach, if the company were to acquire and operate the
computing power needed to find the lowest-cost routes for all trucks
each day, the computing cost would easily wipe out the gains from the
more efficient routes.  No, this idea of enumerating all possible
routes and keeping track of the best, although mathematically sound,
is simply not practical.  Is there a better way to find the
lowest-cost route for each truck?

Nobody knows.  (Or if somebody does know, he or she isn't telling.)
Nobody has found a better way, yet nobody has proven that a better way
cannot exist.  How frustrating is that?

It's more frustrating than you might imagine.  The problem of finding
the lowest-cost routes for brown trucks is better known as the
**traveling-salesman problem**, so-called because in its original
formulation a traveling salesman has to visit $n$ cities, starting and
ending at the same city, and visit all the cities with the shortest
possible tour.  (Sorry about the gendered language.  The name is
historical, and if the problem were first being cast today, I hope
that it would be known as the "traveling-salesperson problem.")  No
algorithm that runs in time $O(n^c)$, for any constant $c$, has ever
been found for the traveling-salesman problem.  We don't know of an
algorithm that, given the intercity distances among $n$ cities, finds
the best possible order to visit the $n$ cities in $O(n^{100})$ time,
$O(n^{1000})$ time, or even $O(n^\mbox{1,000,000})$ time.

It gets worse.  Many problems&mdash;*thousands* of them&mdash;share
this characteristic: for an input of size $n$, we know of no algorithm
that runs in time $O(n^c)$ for any constant $c$, yet nobody has proven
that no such algorithm could exist.  These problems come from a wide
variety of domains&mdash;logic, graphs, arithmetic, and scheduling
among them.

To take the frustration to a whole new level, here's the most amazing
fact:

> If there were an algorithm that ran in $O(n^c)$ time for *any* of
> these problems, where $c$ is a constant, then there would be an
> algorithm that ran in $O(n^c)$ time for *all* of these problems.

We call these problems **NP-complete**.  An algorithm that runs in
time $O(n^c)$ on an input of size $n$, where $c$ is a constant, is a
**polynomial-time algorithm**, so called because $n^c$ with some
coefficient would be the most significant term in the running time.
We know of no polynomial-time algorithm for any NP-complete problem,
but nobody has proven that it's impossible to solve some NP-complete
problem in polynomial time.

And there's even more frustration: many NP-complete problems are
almost the same as problems that we know how to solve in polynomial
time.  Just a small tweak separates them.  For example, there's a well
known algorithm that finds shortest paths from a single start vertex
in a weighted, directed graph, even if the graph has negative-weight
edges, in $O(nm)$ time, where the graph has $n$ vertices and $m$
edges.  If the graph is given as adjacency lists, then the input size
is $O(n+m)$.  Let's assume that $m \geq n$; then the input size is
$O(m)$ and $nm \leq m^2$, and so the running time of the algorithm is
polynomial in the input size.  (You can get the same result if $n >
m$.)  So finding *shortest* paths is easy.  You might be surprised to
learn, however, that finding a *longest* acyclic path (that is, a
longest path without cycles) between two vertices is NP-complete.  In
fact, merely determining whether a graph contains a path without
cycles with at least a given number of edges is NP-complete.

As another example of related problems, where one is easy and one is
NP-complete, consider Euler tours and hamiltonian cycles.  Both of
these problems have to do with finding paths in an undirected graph,
so that $(u,v)$ and $(v,u)$ are the same edge.  An **Euler tour** (so
named because the mathematician Leonhard Euler proved in 1736 that it
was not possible to tour the city of K&ouml;nigsberg, Prussia by
crossing every one of its seven bridges exactly once and ending up at
the starting point) starts and ends at the same vertex and visits each
*edge* exactly once, though it may visit each vertex more than once.
A **hamiltonian cycle** (the name honors W. R. Hamilton, who in 1856
described a mathematical game on a graph known as the dodecahedron, in
which one player sticks five pins in any five consecutive vertices and
the other player must complete the path to form a cycle containing all
the vertices) starts and ends at the same vertex and visits each
*vertex* exactly once (except, of course, for the vertex at which it
starts and ends).  If we ask whether an undirected graph has an Euler
tour, the algorithm is remarkably easy: determine the degree of each
vertex.  (Recall: the degree of a vertex is how many edges are
incident on it.)  The graph has an Euler tour if and only if the
degree of every vertex is even.  But if we ask whether an undirected
graph has a hamiltonian cycle, that's NP-complete.  Notice that the
question is not "what is the order of vertices on a hamiltonian cycle
in this graph?" but just the more basic "yes or no: is it possible to
construct a hamiltonian cycle on this graph?"

NP-complete problems come up surprisingly often.  If you are trying to
find a polynomial-time algorithm for a problem that turns out to be
NP-complete, you are likely to be in for a big helping of
disappointment.  The concept of NP-complete problems has been around
since the early 1970s, and people were trying to solve problems that
turned out to be NP-complete (such as the traveling-salesman problem)
well before then.  To date, we don't know whether a polynomial-time
algorithm exists for any NP-complete problem, nor do we know that no
such algorithm can exist.  Many brilliant computer scientists have
spent years on this question without resolving it.

## The classes P and NP and NP-completeness

So far in this course, we've been concerned about differences in
running times such as $O(n^2)$ vs. $O(n\: \log\: n)$.  When we talk
about NP-completeness, however, we'll be happy if an algorithm runs in
polynomial time, so that differences of $O(n^2)$ vs. $O(n\: \log\: n)$
are insignificant.  Computer scientists generally regard problems
solvable by polynomial-time algorithms as "tractable," meaning "easy
to deal with."  If a polynomial-time algorithm exists for a problem,
then we say that this problem is in the **class P**.

At this point, you might be wondering how could we possibly consider a
problem that requires $O(n^{100})$ time as tractable?  For an input of
size $n = 10$, isn't $10^{100}$ a dauntingly large number?  Yes, it
is; in fact, the quantity $10^{100}$ is a googol (the origin of the
name "Google").  Fortunately, we don't see algorithms that take
$O(n^{100})$ time.  The problems in P that we encounter in practice
require much less time.  I've rarely seen polynomial-time algorithms
that take worse than, say, $O(n^5)$ time.  Moreover, once someone
finds the first polynomial-time algorithm for a problem, others often
follow with even more efficient algorithms.  So if someone were to
devise the first polynomial-time algorithm for a problem but it ran in
time $O(n^{100})$, there would be a good chance that others would
follow suit with faster algorithms.

Now suppose that you're given a proposed solution to a problem, and
you want to verify that the solution is correct.  For example, in the
hamiltonian-cycle problem, a proposed solution would be a sequence of
vertices.  In order to verify that this solution is correct, you'd
need to check that every vertex appears in the sequence exactly once,
except that the first and last vertices should be the same, and if the
sequence is $v_1, v_2, v_3, \ldots, v_n, v_1$ then the graph must
contain edges $(v_1, v_2), (v_2, v_3), (v_3, v_4), \ldots, (v_{n-1},
v_n)$ and back around to $(v_n, v_1)$.  You could easily verify that
this solution is correct in polynomial time.  If it is possible to
verify a proposed solution to a problem in time polynomial in the size
of the input to the problem, then we say that this problem is in the
**class NP**.  (You probably surmised that the name P comes from
"polynomial time."  If you're wondering where the name NP comes from,
it's from "nondeterministic polynomial time."  It's an equivalent, but
not quite as intuitive, way of viewing this class of problems.)  We
call the proposed solution a **certificate**, and in order for the
problem to be in NP, the certificate needs to be polynomial in the
size of the input to the problem.

If you can solve a problem in polynomial time, then you can certainly
verify a certificate for that problem in polynomial time.  In other
words, every problem in P is automatically in NP.  The
reverse&mdash;is every problem in NP also in P?&mdash;is the question
that has perplexed computer scientists for all these years.  We often
call it the "P = NP? problem."

The NP-complete problems are the "hardest" in NP.  Informally, a
problem is **NP-complete** if it satisfies two conditions: (1) it's in
NP and (2) if a polynomial-time algorithm exists for the problem, then
there is a way to convert *every* problem in NP into this problem in
such a way as to solve them all in polynomial time.  If a
polynomial-time algorithm exists for *any* NP-complete
problem&mdash;that is, if any NP-complete problem is in P&mdash;then P
= NP.  Because NP-complete problems are the hardest in NP, if it turns
out that if any problem in NP is not polynomial-time solvable, then
none of the NP-complete problems are.  A problem is **NP-hard** if it
satisfies the second condition for NP-completeness but may or may not
be in NP.

## Decision problems and reductions

So far, I've glossed over one important point about the problems that
we're considering: they are all decision problems.  In other words,
their output is a single bit, indicating "yes" or "no."  I couched
the Euler-tour and hamiltonian-cycle problems in this way: Does the
graph have an Euler tour?  Does it have a hamiltonian cycle?

What about the shortest-path problem?  How can we pose the
shortest-path problem as a yes/no problem?  We can ask "Does the graph
contain a path between two specific vertices whose path weight is at
most a given value $k$?"  We're not asking for the vertices or edges
on the path, but just whether such a path exists.  Assuming that path
weights are integers, we can find the actual weight of the shortest
path between the two vertices.  How?  Pose the question for $k=1$.  If
the answer is no, then try with $k=2$.  If the answer is no, try with
$k=4$.  Keep doubling the value of $k$ until the answer is yes.  If
that last value of $k$ was $k'$, then the answer is somewhere between
$k'/2$ and $k'$.  Then find the true answer by using binary search
with an initial interval of $k'/2$ to $k$.

The second condition for a problem to be NP-complete requires that if
a polynomial-time algorithm exists for the problem, then there is a
way to convert every problem in NP into this problem in such a way as
to solve them all in polynomial time.  Focusing on decision problems,
let's see the general idea behind converting one decision problem,
$X$, into another decision problem, $Y$, such that if there's a
polynomial-time algorithm for $Y$ then there's a polynomial-time
algorithm for $X$.  Here's the idea:

![](reduction-overview.png)

We're given some input $x$ of size $n$ to problem $X$.  We transform
this input into an input $y$ to problem $Y$, and we do so in time
polynomial in $n$, say $O(n^c)$ for some constant $c$.  The way we
transform input $x$ into input $y$ has to obey an important property:
if algorithm $Y$ decides "yes" on input $y$, then algorithm $X$ should
decide "yes" on input $x$, and if $Y$ decides "no" on $y$, then $X$
should decide "no" on $x$.  We call this transformation a
**polynomial-time reduction algorithm**.  Let's see how long the
entire algorithm for problem $X$ takes.  The reduction algorithm takes
$O(n^c)$ time, and its output cannot be longer than the time it took,
and so the size of the reduction algorithm's output is $O(n^c)$.  But
this output is the input $y$ to the algorithm for problem $Y$.  Since
the algorithm for $Y$ is a polynomial-time algorithm, on an input of
size $m$, it runs in time $O(m^d)$ for some constant $d$.  Here, $m$
is $O(n^c)$, and so the algorithm for $Y$ takes time $O((n^c)^d)$, or
$O(n^{cd})$.  Because both $c$ and $d$ are constants, so is $cd$, and
we see that the algorithm for $Y$ is a polynomial-time algorithm.  The
total time for the algorithm for problem $X$ is $O(n^c + n^{cd})$,
which makes it, too, a polynomial-time algorithm.

This approach shows that if problem $Y$ is "easy" (solvable in
polynomial time), then so is problem $X$.  But we'll use
polynomial-time reductions to show that problems are "hard"&mdash;in
particular, NP-hard.  And we'll use reductions to show the reverse: if
problem $X$ is NP-hard, then so is problem $Y$.  Let's suppose that
problem $X$ is NP-hard and that there is a polynomial-time reduction
algorithm to convert inputs to $X$ into inputs to $Y$.  Because $X$ is
NP-hard, there is a way to convert any problem, say $Z$, in NP into
$X$ such that if $X$ has a polynomial-time algorithm, so does $Z$.
Now you know how that conversion occurs, namely by a polynomial-time
reduction:

![](Z-to-X-reduction.png)

Because we can convert inputs to $X$ into inputs to $Y$ with
a polynomial-time reduction, we can expand $X$ as we did earlier:

![](Z-to-X-to-Y-reduction.png)

Instead of grouping the polynomial-time reduction for $X$ to $Y$ and
the algorithm for $Y$ together, let's group the two polynomial-time
reductions together:

![](Z-to-Y-reduction-1.png)

Now we note that if we immediately follow the polynomial-time
reduction for $Z$ to $X$ by the polynomial-time reduction from $X$ to
$Y$, we have a polynomial-time reduction from $Z$ to $Y$:

![](Z-to-Y-reduction-2.png)

Just to make sure that the two polynomial-time reductions in sequence
together constitute a single polynomial-time reduction, we'll use a
similar analysis to what we did before.  Suppose that the input $z$ to
problem $Z$ has size $n$, that the reduction from $Z$ to $X$ takes
time $O(n^c)$, and that the reduction from $X$ to $Y$ on an input of
size $m$ takes time $O(m^d)$, where $c$ and $d$ are constants.  The
output of the reduction from $Z$ to $X$ cannot be longer than the time
it took to produce it, and so this output, which is also the input $x$
to the reduction from $X$ to $Y$, has size $O(n^c)$.  Now we know that
the size $m$ of the input to the reduction from $X$ to $Y$ has size $m
= O(n^c)$, and so the time taken by the reduction from $X$ to $Y$ is
$O((n^c)^d)$, which is $O(n^{cd})$.  Since $c$ and $d$ are constants,
this second reduction takes time polynomial in $n$.

Furthermore, the time taken in the last stage, the polynomial-time
algorithm for $Y$, is also polynomial in $n$.  Suppose that the
algorithm for $Y$ on an input of size $p$ takes time $O(p^b)$, where
$b$ is a constant.  As before, the output of a reduction cannot exceed
the time taken to produce it, and so $p = O(n^{cd})$, which means that
the algorithm for $Y$ takes time $O((n^{cd})^b)$, or $O(n^{bcd})$.
Since $b$, $c$, and $d$ are all constants, the algorithm for $Y$ takes
time polynomial in the original input size $n$.  Altogether, the
algorithm for $Z$ takes time $O(n^c + n^{cd} + n^{bcd})$, which is
polynomial in $n$.

What have we just seen?  We showed that if problem $X$ is NP-hard and
there is a polynomial-time reduction algorithm that transforms an
input $x$ to $X$ into an input $y$ to problem $Y$, then $Y$ is
NP-hard, too.  Because $X$ being NP-hard means that every problem in
NP reduces to it in polynomial time, we picked any problem $Z$ in NP
that reduces to $X$ in polynomial time and showed that it also reduces
to $Y$ in polynomial time.

So now all we have to do to show that a problem $Y$ is NP-complete is

* show that it's in NP, which we can do by showing that there's a way
to verify a certificate for $Y$ in polynomial time, and

* take some other problem $X$ that we know to be NP-hard and give a
polynomial-time reduction from $X$ to $Y$.

There is one more little detail that I've ignored so far: the Mother
Problem.  We need to start with some problem $M$ (the Mother Problem)
that *every* problem in NP reduces to in polynomial time.  Then we can
reduce $M$ to $X$ in polynomial time to show that $X$ is NP-hard,
reduce $X$ to $Y$ in polynomial time to show that $Y$ is NP-hard, and
so on.  Bear in mind, too, that there's no limit on how many other
problems we can reduce a single problem to, so that the family tree of
NP-complete problems starts with the Mother Problem and then branches
out.

## A Mother Problem

Different books list different Mother Problems.  That's fine, since
once you reduce one Mother Problem to some other problem, that other
problem could also serve as the Mother Problem.  We'll use as our
Mother Problem something called **3-CNF satisfiability**.

In 3-CNF satisfiability, we are given a formula with boolean
variables.  Each variable can take on the values 0 and 1, where you
can think of 0 as False and 1 as True.  The operators in the formula
are the logical operators you know from Python, but we write them in
capital letters: AND, OR, and NOT.  We ask whether there is any way to
assign boolean values (0 or 1) to the variables so that the formula
evaluates to 1 (True).  Note that we're not asking what that
assignment of boolean values to the variables is, but just whether
such an assignment exists.

The form of the formula is restricted: it has to be ANDs of
**clauses**, where each clause is an OR of three terms, and each term
is a **literal**: either a variable or the negation of a variable
(such as $\mbox{NOT } x$).  A boolean formula in this form is in
**3-conjunctive normal form**, or **3-CNF**.  For example, the boolean
formula $\left( w \;\mbox{OR } (\mbox{NOT } w) \;\mbox{OR } (\mbox{NOT
} x) \right) \;\mbox{AND } (y \;\mbox{OR } x \;\mbox{OR } z)
\;\mbox{AND } \left((\mbox{NOT } w) \;\mbox{OR } (\mbox{NOT } y)
\;\mbox{OR } (\mbox{NOT } z)\right)$ is in 3-CNF.  Its first clause is
$\left(w \;\mbox{OR } (\mbox{NOT } w) \;\mbox{OR } (\mbox{NOT }
x)\right)$.

Deciding whether a boolean formula in 3-CNF has a satisfying
assignment to its variables is NP-complete.  A certificate is a
proposed assignment of the values $0$ and $1$ to the variables.
Checking a certificate is easy: just plug in the proposed values for
the variables, and verify that the expression evaluates to $1$.  For
3-CNF satisfiability to truly be a Mother Problem, we'd have to show
that every problem in NP reduces to it in polynomial time.  You don't
want to see this proof, at least not in this course.

Here's a frustrating aspect of 3-CNF satisfiability: although it's
NP-complete, there is a polynomial-time algorithm to determine whether
a 2-CNF formula is satisfiable.  A 2-CNF formula is just like a 3-CNF
formula except that it has two literals, not three, in each clause.  A
small change like that takes a problem from being as hard as any
problem in NP to being easy!

## The clique problem

Now we're going to see an interesting reduction, for problems in
different domains: from 3-CNF satisfiability to a problem having to do
with undirected graphs.  A **clique** in an undirected graph $G$ is
a subset $S$ of vertices such that the graph has an edge between every
pair of vertices in $S$.  The **size of a clique** is the number of
vertices it contains.

As you might imagine, cliques play a role in social network theory.
Modeling each individual as a vertex and relationships between
individuals as undirected edges, a clique represents a group of
individuals all of whom have relationships with each other.  Cliques
also have applications in bioinformatics, engineering, and chemistry.

The **clique problem** takes two inputs, a graph $G$ and a positive
integer $k$, and asks whether $G$ has a clique of size $k$.  (I could
have phrased the problem as asking "whether $G$ has a clique of at
least size $k$," but since a clique of size $k$ contains within it
cliques of all sizes less than $k$, the "at least" part is
unnecessary.)  For example, this graph has a clique of size $4$, shown
with heavily shaded vertices, and no other clique of size $4$ or
greater:

![](clique-4.png)

Verifying a certificate is easy.  The certificate is the $k$ vertices
claimed to form a clique, and we just have to check that each of the
$k$ vertices has an edge to the other $k-1$.  This check is easily
performed in time polynomial in the size of the graph.  Now we know
that the clique problem is in NP.

How can a problem in satisfying boolean formulas possibly reduce to a
graph problem?  These two problems seem light years apart.  One is a
problem in boolean logic, and the other is a problem having to do with
graphs.  It's remarkable that we'll be able to reduce 3-CNF
satisfiability to the clique problem, but we will do so.

We start with a boolean formula in 3-CNF.  Suppose that the formula is
$C_1 \;\mbox{AND } C_2 \;\mbox{AND } C_3 \;\mbox{AND } \cdots
\;\mbox{AND } C_k$, where each $C_r$ is one of $k$ clauses.  From this
formula, we will construct a graph in polynomial time, and this graph
will have a $k$-clique if and only if the 3-CNF formula is
satisfiable.  We need to see three things: the construction, an
argument that the construction runs in time polynomial in the size of
the 3-CNF formula, and a proof that the graph has a $k$-clique if and
only if there is some way to assign to the variables of the 3-CNF
formula so make it evaluate to $1$.

To construct a graph from a 3-CNF formula, let's focus on the $r$th
clause, $C_r$.  It has three literals; let's call them $l_1^r$,
$l_2^r$, and $l_3^r$, so that $C_r$ is $l_1^r \;\mbox{AND } l_2^r
\;\mbox{AND } l_3^r$.  Each literal is either a variable or the
negation of a variable.  We create one vertex for each literal, so
that for clause $C_r$, we create a triple of vertices, $v_1^r$,
$v_2^r$, and $v_3^r$.  We add an edge between vertices $v_i^r$
and $v_j^s$ if two conditions hold:

* $v_i^r$ and $v_j^s$ are in different triples; that is, $r$ and $s$
are different clause numbers, and

* their corresponding literals are not negations of each other.

For example, here's a graph that corresponds to the 3-CNF formula
$\left(x \;\mbox{OR } (\mbox{NOT } y) \;\mbox{OR } (\mbox{NOT }
z)\right) \;\mbox{AND } \left((\mbox{NOT } x) \;\mbox{OR } y
\;\mbox{OR } z\right) \;\mbox{AND } (x \;\mbox{OR } y \;\mbox{OR }
z)$:

![](clique-reduction.png)

It's easy enough to see that this reduction can be performed in
polynomial time.  If the 3-CNF formula has $k$ clauses, then it has
$3k$ literals, and so the graph has $3k$ vertices.  At most, each
vertex has an edge to all the other $3k-1$ vertices, and so the number
of edges is at most $3k(3k-1)$, which equals $9k^2 - 3k$.  Since the
number $k$ of clauses is smaller than the size of the input to the
3-CNF satisfiability problem, the graph constructed is polynomial in
the size of the 3-CNF input, and it's easy to determine which edges go
into the graph.

Finally, we need to show that the constructed graph has a $k$-clique
if and only if the 3-CNF formula is satisfiable.  We start by assuming
that the formula is satisfiable, and we'll show that the graph has a
$k$-clique.  If there exists a satisfying assignment, each clause
$C_r$ contains at least one literal $l_i^r$ that evaluates to $1$, and
each such literal corresponds to a vertex $v_i^r$ in the graph.  If we
select one such literal from each of the $k$ clauses, we get a
corresponding set $S$ of $k$ vertices.  I claim that $S$ is a
$k$-clique.  Consider any two vertices in $S$.  They correspond to
literals in different clauses that evaluate to $1$ in the satisfying
assignment.  These literals cannot be negations of each other, because
if they were, then one of them would evaluate to $1$ but the other
would evaluate to $0$.  Since these literals are not negations of each
other, we created an edge between the two vertices when we constructed
the graph.  Because we can pick any two vertices in $S$ as this pair,
we see that there are edges between all pairs of vertices in $S$.
Hence, $S$, a set of $k$ vertices, is a $k$-clique.

Now we have to show the other direction: if the graph has a
$k$-clique $S$, then the 3-CNF formula is satisfiable.  No edges in
the graph connect vertices in the same triple, and so $S$ contains
exactly one vertex per triple.  For each vertex $v_r^i$ in $S$, assign
$1$ to its corresponding literal $l_r^i$ in the 3-CNF formula.  We
don't have to worry about assigning a $1$ to both a literal and its
negation, since the $k$-clique cannot contain edges between vertices
corresponding to a literal and its negation.  Each clause is
satisfied, and so the entire 3-CNF formula is satisfied.  If any
variables don't correspond to vertices in the clique, assign values to
them arbitrarily; they won't affect the formula being satisfied.

In the above example, a satisfying assignment has $y = 0$ and $z = 1$;
it doesn't matter what we assign to $x$.  A corresponding $3$-clique
consists of the heavily shaded vertices, which correspond to
$\mbox{NOT } y$ from clause $C_i$ and $z$ from clauses $C_2$
and $C_3$.

Thus, we have shown that there exists a polynomial-time reduction from
the NP-complete problem of 3-CNF satisfiability to the problem of
finding a $k$-clique.  If you were given a boolean formula in 3-CNF
with $k$ clauses, and you had to find a satisfying assignment for the
formula, you could use the construction we just saw to convert the
formula in polynomial time to an undirected graph, and determine
whether the graph had a $k$-clique.  If you could determine in
polynomial time whether the graph had a $k$-clique, then you would
have determined in polynomial time whether the 3-CNF formula had a
satisfying assignment.  Since 3-CNF satisfiability is NP-complete, so
is determining whether a graph contains a $k$-clique.  As a bonus, if
you could determine not only whether the graph had a $k$-clique, but
which vertices constituted the $k$-clique, then you could use this
information to find the values to assign to the variables of the 3-CNF
formula in a satisfying assignment.

## Reductions galore

I could give you dozens of reductions, each showing that some problem
is NP-complete, but your eyes would quickly glaze over, if they
haven't already.  I wanted to show you the reduction from 3-CNF
satisfiability to the clique problem because it shows how two
problems, on the surface unrelated to each other, are actually the
same problem&mdash;from the point of view of polynomial-time
algorithms.

## Perspective

I've painted quite a gloomy picture here, haven't I?  Imagine a
scenario in which you try to come up with a polynomial-time algorithm
to solve a problem, and no matter how much you press, you just can't
close the deal.  After a while, you'd be thrilled just to find an
$O(n^5)$-time algorithm, even though you know that $n^5$ grows awfully
rapidly.  Maybe this problem is close to one that you know is easily
solved in polynomial time (such as 2-CNF satisfiability vs. 3-CNF, or
Euler tour vs. hamiltonian cycle), and you find it incredibly
frustrating that you can't adapt the polynomial-time algorithm for
your problem.  Eventually you suspect that maybe&mdash;just
maybe&mdash;you've been banging your head against the wall to solve an
NP-complete problem.  And, lo and behold, you are able to reduce a
known NP-complete problem to your problem, and now you know that it's
NP-hard.

Is that the end of the story?  There's no hope that you'll be able to
solve the problem in any reasonable amount of time?

Not quite.  When a problem is NP-complete, it means that *some* inputs
are troublesome, but not necessarily that *all* inputs are bad.
For example, finding a longest acyclic path in a directed graph is
NP-complete, but if you know that the graph is acyclic, then you can
find a longest acyclic path in not just polynomial time, but in
$O(n+m)$ time (where the graph has $n$ vertices and $m$ edges).

The good news goes beyond such pathological special cases.  From here
on, let's focus on optimization problems whose decision variants are
NP-complete, such as the traveling-salesman problem.  Some fast
methods give good, and often very good, results.  The technique of
**branch and bound** organizes a search for an optimal solution into a
tree-like structure, and it cuts off hunks of the tree, thereby
eliminating large portions of the search space, based on the simple
idea that if it can determine that all the solutions emanating from
one node of the search tree cannot be any better than the best
solution found so far, then don't bother checking solutions within the
space represented by that node or anything below it.

Another technique that often helps is **neighborhood search**,
which takes one solution and applies local operations to try to
improve the solution until no further improvement occurs.

Moreover, a host of **approximation algorithms** give results that are
guaranteed to be within a certain factor of the optimal value.  In
many real-world situations, a nearly optimal solution is good enough.
Harkening back to the discussion about the package-delivery company
with brown trucks, they are happy to find nearly optimal routes for
their trucks, even if the routes are not necessarily the best
possible.  Every dollar that they can save by planning efficient
routes helps their bottom line.

Then again, if you're under the impression that NP-complete problems
are the hardest in the world of algorithms, then you've got another
thing coming.  Theoretical computer scientists have defined a large
hierarchy of complexity classes, based on how much time and other
resources are necessary to solve a problem.  Some problems take an
amount of time that is provably exponential in the input size.

And it gets even worse.  For some problems, no algorithm is possible.
That is, there are problems for which it is provably impossible to
create an algorithm that always gives a correct answer.  We call such
problems **undecidable**, and the best-known one is the **halting
problem**, proven undecidable by the mathematician Alan Turing in
1937.  In the halting problem, the input is a computer program A and
the input $x$ to A.  The goal is to determine whether program A,
running on input $x$, ever halts.  That is, does A with input $x$ run
to completion?

Perhaps you're thinking that you could write a program\mdash let's
call it program B&mdash;that reads in program A, reads in $x$, and
simulates A running with input $x$.  That's fine if A on input $x$
actually does run to completion.  What if it doesn't?  How would
program B know when to declare that A will never halt?  Couldn't B
check for A getting into some sort of infinite loop?  The answer is
that although you could write B to check for some cases in which A
doesn't halt, it is provably impossible to write program B so that
*it* always halts and tells you correctly whether A on input $x$
halts.

Because it's not possible to write a program that determines whether
another program running on a particular input even halts, it's also
not possible to write a program that determines whether another
program meets its specification.  How can one program tell whether
another program gives the right answer if it can't even tell whether
the program halts?  So much for perfect automated software testing!
